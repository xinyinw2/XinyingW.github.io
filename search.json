[
  {
    "objectID": "projects/cust_churn.html",
    "href": "projects/cust_churn.html",
    "title": "Customer Churn Prediction",
    "section": "",
    "text": "Overview\nThis project analyzes customer attrition in the telecom sector using the dataset from Kaggle titled “Customer Churn Prediction:Analysis”. The goal is to discover the factors that most strongly relate to attrition and to build prediction models that flag accounts with high risk. The work translates analytic results into actions that support retention planning, resource allocation, and service improvements. Deliverables include clear explanations of churn drivers, prediction outputs for at-risk customers, and visuals that nontechnical partners can use in planning.\n\n\nProblem Statement\nTelecom firms face revenue loss and higher acquisition cost when customers discontinue service. Without timely prediction, teams respond after the customer has already left, which reduces the effect of retention efforts and can damage brand reputation. The objective is to create an analytic workflow that\n\nidentifies the variables that explain attrition with clarity for business partners\npredicts the probability that each active account will discontinue service in the next period\nprovides ranked customer lists with reason codes so teams can act with targeted offers and service interventions\n\n\n\nData\nThis project uses a publicly available telecom customer churn dataset sourced from Kaggle. The dataset provides a comprehensive view of customer behavior and churn in the telecom industry, including customer demographics, service usage, contract attributes, billing amounts, and the churn outcome.\nSnapshot\n\nRecords: 1,000 customers\nFeatures: 10 columns\nTarget: Churn yes or no\n\nPreprocessing summary\n\nKept Churn as the dependent variable\nDropped CustomerID since it carries no predictive signal\nConverted categorical fields to dummy variables for modeling compatibility\nVerified that TotalCharges is approximately MonthlyCharges multiplied by Tenure, with any data quality issues handled during cleaning\n\nFeature dictionary\n\n\n\n\nFeature\n\n\nType\n\n\nDescription\n\n\nRole\n\n\n\n\n\n\nCustomerID\n\n\nID\n\n\nUnique customer key\n\n\nExcluded\n\n\n\n\nAge\n\n\nNumeric\n\n\nCustomer age in years\n\n\nPredictor\n\n\n\n\nGender\n\n\nCategorical\n\n\nMale or Female\n\n\nPredictor\n\n\n\n\nTenure\n\n\nNumeric\n\n\nMonths with provider\n\n\nPredictor\n\n\n\n\nMonthlyCharges\n\n\nNumeric\n\n\nMonthly fee\n\n\nPredictor\n\n\n\n\nContractType\n\n\nCategorical\n\n\nMonth to month, one year, two year\n\n\nPredictor\n\n\n\n\nInternetService\n\n\nCategorical\n\n\nDSL, fiber, or none\n\n\nPredictor\n\n\n\n\nTechSupport\n\n\nCategorical\n\n\nHas support yes or no\n\n\nPredictor\n\n\n\n\nTotalCharges\n\n\nNumeric\n\n\nTotal billed amount\n\n\nPredictor\n\n\n\n\nChurn\n\n\nTarget\n\n\nCustomer left yes or no\n\n\nTarget\n\n\n\n\n\n\nExploratory Data Analysis\nThis section summarizes data shape, missing fields, class balance, and key relationships that explain churn.\n\n\n1. Numeric features distribution\n\nAge Most customers are between thirty and fifty years old with an approximately normal shape.\n\nMonthlyCharges Values range from about twenty to one hundred twenty, reflecting different service tiers.\n\nTotalCharges Values range from zero to around twelve thousand, with most under two thousand. This positive skew suggests many newer or lower cost plans, with a smaller group of long term or premium users.\n\nTenure Spans zero to one hundred twenty months and is concentrated at the lower end, indicating many relatively new customers.\n\n\n\n\n\n\n\n2. Categorical features distribution\n\nContractType Month to month contracts dominate, suggesting a preference for flexibility.\nInternetService Fiber optic is most common.\n\nTechSupport With and without support are close to even.\n\nGender Female is slightly higher than male in this sample.\n\n\n\n\n\n\n\n3. Target variable distribution\n\nThe churn target is imbalanced, with more customers who churned than those who did not. This calls for attention to evaluation choices and possibly class weighting or threshold tuning.\n\n\n\n\n\n\n\n4. Correlation matrix and key relationships\n\nPositive Month to month contracts are strongly associated with higher churn Lack of tech support also aligns with higher churn.\nNegative One year and Two year contracts correlate with lower churn, indicating more loyalty and stability.\n\n\n\n\n\n\n\nTools\nModeling and data: Python (Pandas, scikit learn, matplotlib, seaborn)  Visualization and reporting: Power BI, Excel \n\n\nKey Methods and Approach\nThis study evaluated five supervised learning models to identify the most accurate method for predicting customer churn. The models include K Nearest Neighbors, Naive Bayes, Logistic Regression, Decision Tree, and Random Forest.\nGoal: Select a reliable predictive method for churn risk with clear business interpretation.\nData variants: Compare accuracy with original dataset without preprocessing and cleaned dataset with standard preprocessing.\nEvaluation:\n\nOverall accuracy\n\nPrecision, recall, and F1 score with special attention to the minority class labeled No\n\nPractical usefulness for targeted retention actions\n\n\nK Nearest NeighborsNaive BayesLogistic RegressionDecision TreeRandom Forest\n\n\nObjective\nCompare a K Nearest Neighbors classifier on the original dataset and on a cleaned dataset to see how preprocessing changes accuracy and class level metrics\nBenchmarks\n\nBenchmark 1 original dataset without preprocessing\n\nBenchmark 2 cleaned and preprocessed dataset\n\nEvaluation\n\nAccuracy\n\nPrecision, recall, and F1 with focus on the minority class labeled No\n\n\n\n\nObjective\nAssess Naive Bayes on three dataset variants to understand the impact of preprocessing and class balancing\nBenchmarks\n\nBenchmark 1 original dataset without preprocessing\n\nBenchmark 2 cleaned and preprocessed dataset\n\nBenchmark 3 cleaned dataset balanced with SMOTE\n\nEvaluation\n\nAccuracy\n\nPrecision, recall, and F1 for churned and non churned classes\n\nSensitivity to class imbalance\n\n\n\n\nObjective\nPredict churn using logistic regression and study the effect of preprocessing and threshold tuning on performance and recall\nBenchmarks\n\nBenchmark 1 model on the unprocessed dataset that includes missing values and unencoded categorical fields\n\nBenchmark 2 model after preprocessing that fills missing TotalCharges with the mean, scales numeric fields, and applies one hot encoding to categorical fields such as Gender and ContractType\n\nBenchmark 3 threshold optimization using the ROC curve, selecting a probability cutoff of 0.7 to balance precision and recall\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nBusiness friendly interpretation through model coefficients\n\n\n\n\nObjective\nUse a classification tree to predict churn and provide rules that are easy for partners to understand\nBenchmarks\n\nBenchmark 1 model on the original dataset without preprocessing\n\nBenchmark 2 model on the cleaned and preprocessed dataset\n\nBenchmark 3 model selection via cross validation to choose depth and split settings that generalize well\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nFeature importance to highlight drivers of churn\n\n\n\n\nObjective\nImprove generalization by averaging many trees and quantify gains from preprocessing and tuning\nBenchmarks\n\nBenchmark 1 model on the unprocessed dataset with missing values and unencoded categories\n\nBenchmark 2 model after preprocessing that imputes TotalCharges, standardizes numeric fields, and applies one hot encoding for categories\n\nBenchmark 3 hyperparameter search to find strong settings for number of trees, depth, and split criteria\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nFeature importance to highlight drivers of churn\n\n\n\n\n\n\nResults and Findings\n1. K Nearest Neighbors\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\n\n\n\n\nAccuracy\n\n\n0.84\n\n\n0.92\n\n\n\n\nPrecision for class No\n\n\n0.17\n\n\n0.60\n\n\n\n\nRecall for class No\n\n\n0.80\n\n\n0.94\n\n\n\n\nF1 for class No\n\n\n0.11\n\n\n0.74\n\n\n\n\nClass distribution\n\n\nYes: 39 No: 261\n\n\nYes: 39 No: 261\n\n\n\n\nFindings\n\nCleaning the data improved accuracy by eight percentage points\nOne hot encoding and binning reduced noise and helped the model learn clearer boundaries\nUpsampling addressed class imbalance and produced very large gains in recall and F1 for the minority class No\nThe original data led the model to favor the majority class Yes and missed many true No cases\nThe cleaned data produced a more balanced classifier that is useful for retention actions\n\nOverall KNN performs much better on the cleaned data. Preprocessing and class balancing were essential to identify at risk customers and to achieve balanced performance.\n2. Naive Bayes\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nSMOTE\n\n\n\n\n\n\nAccuracy\n\n\n0.832\n\n\n0.888\n\n\n0.804\n\n\n\n\nPrecision for class Yes\n\n\n0.97\n\n\n0.91\n\n\n0.96\n\n\n\n\nRecall for class Yes\n\n\n0.83\n\n\n0.96\n\n\n0.81\n\n\n\n\nF1 score\n\n\n0.90\n\n\n0.94\n\n\n0.88\n\n\n\n\nClass distribution\n\n\nYes: 27 No: 181\n\n\nYes: 13 No: 159\n\n\nYes: 33 No: 217\n\n\n\n\nFindings\n\nCleaning improved overall accuracy and raised recall for churn class Yes from eighty three percent to ninety six percent\nSMOTE balancing increased recall for the churn class relative to the original data but reduced overall accuracy to zero point eight zero four\nVery high precision on the original data reflects the class imbalance and a conservative decision boundary that missed some true churners\nThe cleaned dataset delivered the best balance of accuracy and class level metrics\nNext steps confirm robustness with stratified cross validation, review calibration, and tune the threshold to align recall with retention goals\n\n3. Logistic Regression\n\n\n\n\nBenchmark\n\n\nAccuracy\n\n\nPrecision Class 0\n\n\nRecall Class 0\n\n\nF1 score Class 0\n\n\nClass distribution\n\n\n\n\n\n\nWithout preprocessing\n\n\n0.94\n\n\n0.65\n\n\n0.42\n\n\n0.51\n\n\nYes: 88 No: 12\n\n\n\n\nWith preprocessing default\n\n\n0.95\n\n\n0.87\n\n\n0.79\n\n\n0.83\n\n\nYes: 87 No: 13\n\n\n\n\nWith preprocessing optimized threshold\n\n\n0.92\n\n\n0.63\n\n\n1.00\n\n\n0.78\n\n\nYes: 87 No: 13\n\n\n\n\nFindings\n\nCleaning and encoding improved accuracy from zero point ninety four to zero point ninety five and lifted recall for non churn class zero from forty two percent to seventy nine percent\nThreshold tuning increased recall for class zero to one hundred percent with a tradeoff in precision and overall accuracy to zero point ninety two\nRemoving missing values and encoding categories stabilized training and produced more reliable probabilities\nPreprocessed default settings give the best overall balance of accuracy and class level metrics, while the optimized threshold is useful when recall for non churn must be maximized for a specific business rule\n\n4. Decision Tree\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nCross-validation\n\n\n\n\n\n\nAccuracy\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nPrecision for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nRecall for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nF1 for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nClass distribution\n\n\nYes: 27 No: 223\n\n\nYes: 32 No: 144\n\n\nYes: 30 No: 150\n\n\n\n\nFindings\n\nAccuracy, precision, recall, and F1 were all one hundred percent on both original and cleaned data\nHeavy class imbalance with about eighty eight percent Yes likely inflated the metrics and reduced generalization\nPreprocessing did not change the winning features, but it shifted their relative importance\n\n5. Random Forest\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nHyperparameter tuning\n\n\n\n\n\n\nAccuracy\n\n\n0.96\n\n\n1.0\n\n\n0.996\n\n\n\n\nPrecision\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nRecall\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nF1 score\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nTrain score\n\n\n1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nTest score\n\n\n0.96\n\n\n1.0\n\n\n0.996\n\n\n\n\nClass distribution\n\n\nYes: 27No: 223\n\n\nYes: 33No: 217\n\n\nYes: 33No: 216\n\n\n\n\nFindings\n\nPreprocessing raised accuracy from zero point ninety six to one point zero and produced perfect precision, recall, and F1\nFilling missing TotalCharges and encoding categories reduced noise and likely helped the trees form cleaner splits\nClass balance shifted slightly toward Yes after cleaning, yet the target remains imbalanced near eighty six percent Yes\nPerfect metrics across train and test suggest limited stress on generalization; validate with stratified cross validation and report balanced accuracy and AUC\nHyperparameter tuning delivered accuracy of zero point nine nine six with other metrics unchanged, confirming a strong and stable model while still warranting checks for robustness on unseen data\n\n\n\nConclusion\nOur comparison shows that Decision Tree and Random Forest reached near perfect accuracy, while KNN and Logistic Regression also performed strongly at around the low to mid nineties. Naive Bayes trailed at about eighty eight percent on the cleaned data. Cross validation and confusion matrix checks did not indicate overfitting for the tree models, though the class imbalance means we should continue to validate with balanced accuracy and AUC on true holdouts.\nFor production use, pair one tree based model with one non tree model to balance strengths and reduce risk from unseen patterns. For example, deploy a Decision Tree or Random Forest alongside a KNN or Logistic Regression model, monitor both, and choose actions when they agree or when calibrated churn risk exceeds a threshold. With these models, a telecom company can flag at risk customers early and trigger targeted retention steps such as personalized discounts, proactive service outreach, and plan reviews to protect revenue and improve satisfaction.\n\n\nBusiness Implication\n\n\nRevenue and retention\n\nReduce churn through early outreach\nLift lifetime value with right sized plans\nShift spend from broad ads to focused saves\n\nOperations and service\n\nRoute high risk accounts to priority care\nTrigger proactive outreach after tickets or outages\nSet stronger service targets for high risk segments\n\n\nMarketing and offers\n\nPersonalize incentives by churn driver\nRun win back campaigns on persisting risk\nUse uplift testing to target customers who respond\n\nPlanning and governance\n\nForecast churn driven revenue and program ROI\nMonitor fairness by segment and keep decision logs\nIntegrate scores into CRM with clear reason codes\n\n\n\n\n\nNext Steps\nOur results are strong, but the target class is very imbalanced and Naive Bayes underperformed. This limits confidence in real world performance and suggests that the current metrics may be inflated by class skew. To help improve our models, we should consider the following steps:\n\nData and labeling\n\nCollect more recent records to reduce class skew and reflect current behavior\nBalance the target using resampling or class weights and compare to the current baseline\nAudit labels and remove duplicates that can inflate accuracy\n\nValidation and metrics\n\nUse stratified cross validation with a true holdout set\nAdd balanced accuracy, Matthews correlation, AUC ROC, and AUC PR\nUse time based validation if the data has a natural timeline\n\nModel robustness and calibration\n\nCalibrate probabilities for better threshold control\nTune thresholds by segment to meet business goals such as higher recall for churners\nFor Naive Bayes, add interaction features or consider a variant that handles continuous features more flexibly\nAdd a gradient boosted tree as an additional comparator\n\nFeature quality and leakage checks\n\nStandardize numeric fields and encode categorical fields consistently\nKeep binning only where it helps\nRecheck for target leakage and confirm drivers with permutation importance and SHAP\n\nMonitoring and operations\n\nTrack live KPIs such as monthly churn rate, save rate, net revenue saved, offer cost per save, and calibration error\nLog predictions with reason codes in the CRM so agents see top drivers\nSchedule retraining and drift checks on a regular cadence\n\nSummary action\n\nDeploy one tree model and one non tree model in parallel\nCalibrate and monitor both, then re evaluate once additional balanced data is available\n\n\n\n\nLinks"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Job Recommendation System\n\n\n\nPython\n\nNLP\n\nRecommendation\n\n\n\nMatches candidates to roles using a large LinkedIn Jobs and Skills dataset, with skill based similarity and TF-IDF methods to return the most relevant openings.\n\n\n\nMar 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeart Attack Prediction\n\n\n\nR\n\nTableau\n\nClassification\n\n\n\nPredicts elevated heart-attack risk using machine learning and explains key drivers to support prevention and clinical decisions.\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Churn Prediction\n\n\n\nPython\n\nPower BI\n\nClassification\n\n\n\nPredicts which telecom customers are most likely to leave and explains the key drivers, using supervised models and clear visuals so teams can act on retention quickly.\n\n\n\nDec 10, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xinying Wu",
    "section": "",
    "text": "Hello! Welcome my portfolio website."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Xinying Wu",
    "section": "About Me",
    "text": "About Me\nMaster of Science in Business Analytics candidate at the University of California, Irvine’s Paul Merage School of Business with a strong foundation in Business Administration and Computer Information Science from the University of Oregon.\nI bring hands-on experience in data analytics, process optimization, and data-driven decision support. As a Business Analyst at Edwards Lifesciences, I developed Python-based automation models to detect anomalies in 190K+ regulatory distribution records, applied SQL and machine learning for pattern detection, and visualized insights through Tableau to enhance data quality oversight.\nMy academic projects include designing SQL-based financial dashboards to assess credit card fraud risk, predicting heart attack probability using R-based machine learning models, and modeling telecom customer churn with Python and Power BI.\nProficient in Python, R, SQL, Tableau, Power BI, and Excel, I excel at transforming raw data into actionable insights that improve business performance, strengthen compliance, and guide strategic decisions. Passionate about leveraging analytics to drive operational efficiency and innovation across organizations."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Xinying Wu",
    "section": "Education",
    "text": "Education\nUniversity of California, Irvine | Irvine, CA\nM.S. in Business Analytics | Jul. 2024 - Jun. 2025\nThe University of Oregon | Eugene, OR\nB.S. in Business Administration | Sep. 2018 - Jun. 2022\nMinor in Computer Information Science | Sep. 2018 - Jun. 2022"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Xinying Wu",
    "section": "Experience",
    "text": "Experience\nEdwards Lifesciences, Irvine, CA | Business Analyst | Jan. 2025 - Jun. 2025\n\n• Built Python automation models and applied unsupervised machine learning with SQL to detect anomalies in 190K+ regulatory distribution records, improving data integrity and supply chain quality oversight.\n• Created Tableau and Excel dashboards to visualize error-prone categories, conduct gap analysis, and deliver actionable insights that guided operational improvements.\n• Contributed to data governance documentation and collaborated with Edwards stakeholders through weekly reviews to refine strategies aligned with compliance and business goals.\n\nGEO.S.BUSH & CO, Portland, OR | International Trade Representative | Oct. 2022 - Apr. 2024\n\n• Managed end-to-end logistics documentation to ensure timely customs clearance, regulatory compliance, and smooth operational workflows.\n• Audited shipment data against purchase orders and submitted compliance documents to maintain data accuracy, support integrity, and minimize delivery delays.\n• Collaborated with internal teams and external partners to resolve shipment discrepancies and streamline processes, demonstrating strong organizational and problem-solving skills."
  },
  {
    "objectID": "projects/heart.html",
    "href": "projects/heart.html",
    "title": "Heart Attack Prediction",
    "section": "",
    "text": "Overview\nThis project develops a predictive model to flag individuals at elevated risk of heart attack, combining machine learning with interpretable analytics so results are useful for screening, prevention, and clinical decision support. We focus on identifying which clinical, lifestyle, and demographic factors most influence risk, and on calibrating the model’s precision and recall to be reliable for real-world use.\nWhy this matters\n\nHeart disease is a leading cause of death worldwide (18M+ deaths annually).\n\nIn the U.S., someone has a heart attack every ~40 seconds, and over 50% report no prior symptoms.\n\nThe economic burden exceeds $200B annually.\n\nWho benefits\n\nPatients: personalized risk scores and driver explanations to guide lifestyle changes, medication adherence, and timely medical follow-up.\n\nClinicians: an evidence-based complement to clinical judgment that helps prioritize care and manage panels efficiently.\n\nPolicymakers & health systems: population-level insights to target prevention programs and reduce cardiovascular burden.\n\nResearch questions\n\nWhich factors most strongly influence heart-attack risk in our data?\n\nWhat precision/recall balance makes the model dependable for screening and referral?\n\nHow can insights translate into preventive actions for patients (e.g., lifestyle changes, adherence)?\n\n\n\nProblem statement\nCardiovascular disease is a leading cause of death, and many heart attacks occur without prior symptoms. Screening resources are limited, and risk is often underestimated until it is too late. Patients need clear, interpretable feedback they can act on, and clinicians need a reliable way to identify elevated risk earlier and prioritize preventive care. The core problem is to build a model that estimates individual heart-attack risk from routinely available data and explains why a person is high risk in time to intervene.\nOur objectives are to predict near-term heart-attack risk at the individual level, explain the top risk drivers in language patients and clinicians understand, and calibrate precision and recall so the model is safe for screening (high recall) and practical for follow-up (adequate precision). The scope includes clinical, lifestyle, and demographic inputs commonly captured in care or surveys, and concise contributing factors. We must handle class imbalance and missing data while maintaining interpretability and fairness across subgroups.\nSuccess will be defined by meeting agreed performance thresholds (e.g., strong AUROC and PR-AUC) and an operational precision/recall target set with clinical stakeholders. We will also look for stable performance across age and sex subgroups, documented bias checks, and explanation quality that clinicians rate as useful for counseling and care planning.\n\n\nData\nThe dataset comes from a dataset named “Heart Attack Risk Prediction Dataset” from Kaggle. This dataset is designed to analyze and predict heart attack risk based on various health, lifestyle, and demographic factors. It includes attributes such as age, cholesterol levels, blood pressure, smoking habits, exercise patterns, dietary preferences, and medical history. By leveraging predictive analytics and machine learning, this dataset can aid researchers and healthcare professionals in developing proactive strategies for heart disease prevention and management. .\n\nSnapshot\n\nRows: 8,763\nColumns: 26\nTarget variable: Heart_Attack_Risk (binary: 1 = At Risk, 0 = No Risk)\nPredictors: the modeling dataset includes 24 features across demographics, clinical/physiologic measures, medical history, and lifestyle—with Patient ID excluded from the final set\n\n\n\nData Dictionary\n\n\n\n\nFeature\n\n\nType\n\n\nDescription\n\n\n\n\n\n\nPatient ID\n\n\nQL(Unique)\n\n\nUnique identifier for each patient\n\n\n\n\nAge\n\n\nQ(Int)\n\n\nAge in years\n\n\n\n\nSex\n\n\nQL(2)\n\n\nMale or Female\n\n\n\n\nCholesterol\n\n\nQ(Int)\n\n\nTotal cholesterol (mg/dL)\n\n\n\n\nBlood Pressure\n\n\nQL(2)\n\n\nSystolic / Diastolic (mmHg)\n\n\n\n\nHeart Rate\n\n\nQ(Int)\n\n\nResting heart rate (bpm)\n\n\n\n\nDiabetes\n\n\nQL(Bin)\n\n\nDiabetes status (Yes/No)\n\n\n\n\nFamily History\n\n\nQL(Bin)\n\n\nFamily history of heart problems (1 Yes / 0 No)\n\n\n\n\nSmoking\n\n\nQL(Bin)\n\n\nSmoking status (1 Smoker / 0 Non-smoker)\n\n\n\n\nObesity\n\n\nQL(Bin)\n\n\nObesity status (1 Obese / 0 Not obese)\n\n\n\n\nAlcohol Consumption\n\n\nQL(4)\n\n\nNone / Light / Moderate / Heavy\n\n\n\n\nExercise Hours Per Week\n\n\nQ(Real)\n\n\nHours of exercise per week\n\n\n\n\nDiet\n\n\nQL(3)\n\n\nDiet quality (Healthy / Average / Unhealthy)\n\n\n\n\nPrevious Heart Problems\n\n\nQL(Bin)\n\n\nPrior heart-related issues (1 Yes / 0 No)\n\n\n\n\nMedication Use\n\n\nQL(Bin)\n\n\nTakes medication (1 Yes / 0 No)\n\n\n\n\nStress Level\n\n\nQ(Int)\n\n\nSelf-reported stress (1–10)\n\n\n\n\nSedentary Hours Per Day\n\n\nQ(Real)\n\n\nAverage sedentary hours per day\n\n\n\n\nIncome\n\n\nQ(Real)\n\n\nIncome level\n\n\n\n\nBMI\n\n\nQ(Real)\n\n\nBody Mass Index\n\n\n\n\nTriglycerides\n\n\nQ(Int)\n\n\nTriglycerides (mg/dL)\n\n\n\n\nPhysical Activity Days Per Week\n\n\nQ(Int)\n\n\nDays with physical activity (per week)\n\n\n\n\nSleep Hours Per Day\n\n\nQ(Real)\n\n\nAverage sleep hours per day\n\n\n\n\nCountry\n\n\nQL(N)\n\n\nCountry of residence\n\n\n\n\nContinent\n\n\nQL(7)\n\n\nContinent of residence\n\n\n\n\nHemisphere\n\n\nQL(2)\n\n\nNorthern / Southern\n\n\n\n\nHeart Attack Risk\n\n\nQL(Bin)\n\n\nTarget label (1 Yes / 0 No)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotation used\nQ(Int) = Quantitative (Integer)\nQ(Real) = Quantitative (Real)\nQL(Bin) = Qualitative (Binary)\nQL(N) = Qualitative (Categorical) with N levels\n\n\n\n\nData Preprocessing\n\nColumns\n\nDrop Patient.ID\nMove Heart.Attack.Risk to the first column\n\nMissing values and outliers\n\nChecked with a z-score screen\n\nNone detected (no imputation or trimming applied)\n\nBlood pressure parsing\n\nSplit Blood.Pressure into Systolic and Diastolic\n\nExample: 158/88 → Systolic = 158, Diastolic = 88\n\nEncode categorical variables\n\nBinary to 0/1\n\nSex → Female (1 = Female, 0 = Male)\nHemisphere → Northern.Hemisphere (1 = Northern, 0 = Southern)\nOthers were kept in numeric form when building a correlation matrix before being converted to factors for training models\n\nMulti-category (one-hot dummies)\n\nCreated dummies for Diet, Country, Continent\nConverted to numeric first when building a correlation matrix\nConverted to factors and finally dummies for training models\n\n\nScale numeric features\n\nStandardize all numeric predictors to mean = 0 and standard deviation = 1\n\n\n\n\nDataset versions and splits\nTwo final clean datasets (same row split, different handling of multi-category variables)\n\ndata: 26 columns, multi-category variables kept as factors\n\ndata1: 52 columns, multi-category variables dummy-encoded\n\n\n\n\n\nDataset\n\n\nColumns\n\n\nTotal rows\n\n\nTrain rows\n\n\nTest rows\n\n\n\n\n\n\ndata\n\n\n26\n\n\n8,763\n\n\n3,138\n\n\n4,382\n\n\n\n\ndata1\n\n\n52\n\n\n8,763\n\n\n3,138\n\n\n4,382\n\n\n\n\nWhich dataset for which models\n\nNumeric-only algorithms: use data1 (dummy-encoded) — e.g., k-NN, SVM\nModels that accept factors in our toolchain: use data (factor-based) — e.g., Logistic Regression, Naive Bayes, Decision Tree, Random Forest (Using factors also yields clearer tree visualizations.)\n\nClass distribution strategy\n\nOriginal distribution: ~36% “At risk”, ~64% “No risk”\nTest set (stratified): kept the original distribution (~36/64) to reflect real world prevalence\nTraining set (balanced): adjusted to 50% / 50% (“At risk” / “No risk”) to reduce class-imbalance bias during learning\n\nNotes\n\nBoth datasets share the same train/test split, ensuring fair model comparisons.\n“Balanced” training was achieved by resampling so each class contributes equally."
  },
  {
    "objectID": "projects/job_rec.html",
    "href": "projects/job_rec.html",
    "title": "Job Recommendation System",
    "section": "",
    "text": "Overview\nThis project builds a skill based job recommendation system using a 1.3 million record LinkedIn jobs and skills dataset from 2024. The pipeline cleans and normalizes skills, represents text with TF-IDF, computes similarity to a users input profile, and returns a ranked list of relevant roles. The system shortens search time for candidates, helps recruiters target the right audience, and keeps job platforms engaging through accurate and transparent matches. Deliverables include a working prototype and a report that explains data preparation, model design, evaluation, and sample recommendations.\n\n\nProblem Statement\nRecruiters must screen large volumes of resumes for each opening, which leads to slow hiring cycles, inconsistent evaluations, and missed qualified candidates. Manual review struggles with unstructured resume formats, varied terminology, and subtle skill equivalence, and it increases the risk of unconscious bias and poor candidate experience.\nThe goal is to build a resume recommendation system that ingests a job description and a pool of resumes, then ranks and surfaces the most relevant candidates with clear reasons. The system should handle noisy text and diverse formats, map skills and experience to job requirements, and provide fast, explainable results that support fair and data driven decisions. Success will be measured by higher precision at the top of the ranked list, improved recall of qualified candidates, reduced time to screen, and stable fairness metrics across candidate groups.\n\n\nData\nThis project uses the LinkedIn Jobs and Skills dataset named “1.3M Linkedin Jobs & Skills (2024)” from Kaggle. The dataset contains about 1.3 million job listings from the 2024 with 2 fields. Each post has a link and a list of skills. For faster testing, we worked with a smaller sample of 50,000 rows and kept the full data for final checks. The file is loaded and cleaned in Google Colab.\nSnapshot\n\nRows in full set 1,296,381\n2 columns: job_link and job_skills\njob_link: the URL for the job post\njob_skills: a comma separated list of required or preferred skills\nWorking subset 50,000 rows\n\nData preprocessing\nCreate clean, consistent skills text that is ready for search, matching, and vectorization.\nSteps\n\nLoad and inspect read the CSV, check row count, columns, and missing values\n\nRemove noise drop rows with missing job_skills and remove duplicates so each job appears once\n\nNormalize text convert to lowercase, trim extra spaces, remove punctuation and special characters while keeping commas that separate skills\n\nTokenize skills split job_skills by commas so each skill becomes a clean token\n\nStopwords remove common words that add no meaning and a custom list such as skill, experience, degree\n\nLemmatize use spaCy to reduce words to their base form for example analyzing becomes analyze\n\nStandardize names build a skills mapping dictionary so variants map to one canonical name for example ms excel and microsoft excel become Excel\n\nReassemble join cleaned tokens into a standardized skills string for each job\n\nVectorize create Bag of Words and TF IDF features for ranking and retrieval, with embeddings as a future option\n\nQuality Check\n\nConfirm job_link remains unique\n\nRecount missing values after cleaning\n\nReview top skill frequencies before and after to verify standardization\n\nTime the pipeline on the 50,000 row sample to ensure efficient runs\n\n\n\nExploratory Data Analysis\n\n\n1. Wordcloud\n\nSoft skills dominate. Terms like communication, customer service, time management, and problem solving appear most often, signaling that employers value people who can interact well, manage time, and resolve issues across roles and industries.\n\nEducation signals. High school diploma appears frequently as a baseline; bachelor’s degree is present but less common, suggesting only some roles explicitly require higher education.\n\nDigital literacy. Tools such as Microsoft Office and Office suites show up strongly, indicating basic tech proficiency is a standard expectation.\n\nTakeaway: The market emphasizes transferable soft skills, basic digital tools, and a broad entry-level educational bar, which together shape how candidates should present skills and how platforms should weight features in recommendations.\n\n\n\n\n\n\n2. Top 20 Most Common Skills\n\nInterpersonal skills lead. Management, communication, and service appear most often, reinforcing the importance of collaboration and people skills across roles.\n\nIndustry-specific skills. Terms like medical, health, safety, and license point to healthcare and compliance needs where certifications are common.\n\nTechnical depth. System, engineering, and development highlight demand for technical expertise in software, infrastructure, and industrial roles.\n\nBusiness execution. Project, plan, teamwork, and analysis reflect strategic planning, delivery, and cross-functional collaboration.\n\nTakeaway: Employers want a mix of people skills, recognized credentials, technical capability, and analytical execution—with strong signals from healthcare, engineering, business management, insurance, and technology.\n\n\n\n\n\n\n3. Top 20 Most Common Skills in Analytics Roles\n\nCore soft skills. Analysis, management, and communication are prominent, showing analysts must interpret data and work effectively with teams.\n\nFinancial fluency. Financial, cost, budget, and accounting point to forecasting, cost analysis, budgeting, and reading financial statements to support decisions.\n\nTool basics. Microsoft and Excel signal baseline proficiency with spreadsheets and reporting workflows.\n\nOperational execution. Report, process, and system suggest compiling large datasets, standardizing processes, and communicating findings clearly.\n\nTakeaway: Analytics roles blend data interpretation, business finance, and clear communication, with everyday tooling in Excel and structured reporting.\n\n\n\n\n\n\nTools\nModeling and data: Python (Pandas, scikit learn, matplotlib, seaborn, re, nltk, spacy, wordcloud, matplotlib.pyplo, streamlit,t, flask, pyngrok)  Visualization and reporting: Excel  User interface: Bolt\n\n\nKey Methods and Approach\n1. Training pipeline\n\nBaseline TF-IDF + CosineBaseline Word2Vec + Clustering\n\n\n\nRepresent job skills and user profile with TF-IDF vectors\n\nRank jobs by cosine similarity to the user profile\n\nStrengths fast, transparent, easy to explain and tune\n\n\n\n\nEmbed skills with Word2Vec to capture semantic similarity\n\nCluster jobs and retrieve nearest clusters for the user profile\n\nStrengths handles synonyms and near-matches better than pure TF-IDF\n\n\n\n\n2. Hyperparameter tuning\n\nUse grid search, random search, or Bayesian optimization for key settings\n\nTF-IDF n-grams, min_df, max_df\n\nCosine retrieval top-K cutoff\n\nWord2Vec vector size, window, min_count, epochs\n\n\nTrack results with a simple experiment log for reproducibility\n\n3. Validation\n\nk-fold cross-validation for robust estimates\n\nHoldout set for a final unbiased check\n\nOptional A/B testing for UI-level comparisons once a prototype is live\n\n4. Model Evaluation Metrics\n\nPrecision, Recall, F1-score for relevance at the top of the list\n\nAUC-ROC for ranking quality (where labels exist)\n\nRMSE only when evaluating numeric relevance scores against ground truth\n\n5. Error analysis\n\nReview mis-ranked or missed jobs, and high-error segments\n\nCheck failure patterns by industry, seniority, and skill sparsity\n\nUse insights to refine cleaning, skill standardization, and feature choices\n\n6. User interface\n\nBolt UI with a LinkedIn-style layout for familiarity\n\nInputs user skill list and optional role or industry\n\nOutputs ranked jobs with top matching skills highlighted and a link to the posting\n\n7. Feedback loop\n\nCollect quick thumbs-up or thumbs-down to improve future rankings\n\nLog queries and outcomes to support retraining and threshold tuning\n\n\n\nModel Training\nGoal\nTrain and fine tune NLP models that deliver accurate, explainable job recommendations.\n1. Text vectorization (TF–IDF)\nWhat it is:\nVectorization turns text into numbers so models can learn. We considered One-Hot, Bag of Words, TF–IDF, and word embeddings (Word2Vec, GloVe, FastText). We use TF–IDF on the job_skills column.\nWhy TF–IDF:\n\nDown-weights common skills like “communication” or “teamwork,” reducing noisy matches\nStronger signal than BoW, which treats every token equally\nMore efficient than embeddings for our scope no large pre-trained models or extra compute\n\nSteps:\n\nBuild a TF–IDF model to convert job skills into numeric vectors\n\nGet the feature names (unique skills)\n\nTransform skills into a sparse TF–IDF matrix\n\nConvert to a Pandas DataFrame for ranking and retrieval\n\n2. Building the recommendation model:\nInputs:\n\nJobs LinkedIn postings from Kaggle with titles, descriptions, and skills\n\nResumes uploaded DOCX files with experience and skills\n\nPipelines:\n\nJob pipeline\n\nExtract title, description, and skills\n\nChunk long descriptions if needed\n\nVectorize with TF–IDF (and optional embeddings for future scale)\n\nStore job vectors in a vector database for fast lookup\n\n\nResume pipeline\n\nParse DOCX text and clean it\n\nNormalize skills to the same schema as jobs\n\nVectorize the resume to the same space as jobs\n\nStore resume vectors in the same index\n\n\nRetrieval and ranking:\n\nSimilarity metric use cosine similarity between the resume vector and job vectors\n\nRanking sort by similarity and return the top five matches with key skills highlighted\n\nFeedback loop:\n\nCollect thumbs-up or thumbs-down on recommendations\n\nRe-rank based on feedback and log interactions to improve future results\n\n3. Deployment and testing\nTest design:\nWe gathered a diverse set of resumes across multiple industries to check how well the system handles different backgrounds and wording. Sectors included Technical and Software, Healthcare and Medical, Education and Childcare, Creative and Design, and Business and Management.\nPrototype and UI:\nThe first UI was built with Bolt and deployed as a clickable prototype for rapid iteration:\nOpen the prototype\nUser test procedure:\n\nUpload resume (DOCX) into the mock UI\n\nExtract skills and normalize them to the same schema as job postings\n\nRetrieve matches using cosine similarity over the vector index\n\nReview Top-5 recommendations with matched skills highlighted\n\nRecord feedback on relevance and clarity for later re-ranking\n\nWhat we evaluated:\n\nSkill extraction accuracy do extracted skills reflect the resume content\n\nRecommendation quality are the top results relevant and varied\n\nUser experience is the flow clear, fast, and easy to use on first try\n\nExample walkthrough Healthcare resume\nWe uploaded a Healthcare and Medical resume and the system returned five recommended roles ranked by similarity. The first result was the closest match, with subsequent items offering adjacent options for exploration.\n\n\n\n\n\n\n\n\n\nResults\n\n\n\n\n\n\nNoteCurrent performance\n\n\n\nOffline accuracy: 85% on a held-out validation set (cross-validated).\n\n\nInitial findings\nEarly tests showed inaccurate recommendations. Two drivers stood out\n\nthe model was in an early stage\n\nthe Bolt prototype used a very small set of about fifteen job postings\n\nModel improvements\n\nAdded Word2Vec based matching to capture semantic similarity between skills\n\nExample cleaning and sanitation are treated as related, improving match quality\n\n\nKept TF–IDF for speed and transparency, and blended scores where helpful\n\nData freshness plan\n\nImplement periodic scraping of job listings with BeautifulSoup or Scrapy\n\nConsider API based feeds where available to reduce scraping overhead\n\nDefine a refresh schedule and deprecate stale jobs after a set age\n\nInterim augmentation (prototype phase)\n\nUsed manual uploads to add recent postings and diversify test data\n\nLogged outcomes to identify gaps in skills coverage and labeling\n\nWhat to track next\n\nClick-through and thumbs-up / thumbs-down rates in the UI\n\nCoverage of top skills by industry and the age of postings in the index\n\n\n\nBusiness Implication\nOur system shortens hiring cycles and lowers costs by automating first-pass screening, so recruiters spend time on fewer, higher-quality resumes. Using TF-IDF and Word2Vec to match skills and context improves shortlist precision and interview pass-through rates, while ongoing feedback tuning keeps results aligned with recruiter intent.\nBecause job listings are refreshed through automated scraping, recommendations stay current with market demand and scale cleanly as volume grows. The pipeline’s consistent text processing and explainable matching reduce subjective bias, supporting fairer, more defensible decisions. Candidates also benefit from clearer “why this match” signals and more relevant roles, which boosts engagement and reduces drop-off.\nFinally, product analytics—queries, matches, clicks, and feedback—surface skill gaps, channel performance, and conversion bottlenecks for workforce planning. Together, these capabilities differentiate the platform for both employers and job seekers. Core KPIs to monitor include time-to-screen, time-to-offer, precision@K, candidate response rate, cost per hire, and fairness metrics across groups.\n\n\nLinks"
  }
]
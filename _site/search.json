[
  {
    "objectID": "projects/cust_churn.html",
    "href": "projects/cust_churn.html",
    "title": "Customer Churn Prediction",
    "section": "",
    "text": "Overview\nThis project analyzes customer attrition in the telecom sector using the dataset from Kaggle titled “Customer Churn Prediction:Analysis”. The goal is to discover the factors that most strongly relate to attrition and to build prediction models that flag accounts with high risk. The work translates analytic results into actions that support retention planning, resource allocation, and service improvements. Deliverables include clear explanations of churn drivers, prediction outputs for at-risk customers, and visuals that nontechnical partners can use in planning.\n\n\nProblem Statement\nTelecom firms face revenue loss and higher acquisition cost when customers discontinue service. Without timely prediction, teams respond after the customer has already left, which reduces the effect of retention efforts and can damage brand reputation. The objective is to create an analytic workflow that\n\nidentifies the variables that explain attrition with clarity for business partners\npredicts the probability that each active account will discontinue service in the next period\nprovides ranked customer lists with reason codes so teams can act with targeted offers and service interventions\n\n\n\nData\nThis project uses a publicly available telecom customer churn dataset sourced from Kaggle. The dataset provides a comprehensive view of customer behavior and churn in the telecom industry, including customer demographics, service usage, contract attributes, billing amounts, and the churn outcome.\nSnapshot\n\nRecords: 1,000 customers\nFeatures: 10 columns\nTarget: Churn yes or no\n\nPreprocessing summary\n\nKept Churn as the dependent variable\nDropped CustomerID since it carries no predictive signal\nConverted categorical fields to dummy variables for modeling compatibility\nVerified that TotalCharges is approximately MonthlyCharges multiplied by Tenure, with any data quality issues handled during cleaning\n\nFeature dictionary\n\n\n\n\nFeature\n\n\nType\n\n\nDescription\n\n\nRole\n\n\n\n\n\n\nCustomerID\n\n\nID\n\n\nUnique customer key\n\n\nExcluded\n\n\n\n\nAge\n\n\nNumeric\n\n\nCustomer age in years\n\n\nPredictor\n\n\n\n\nGender\n\n\nCategorical\n\n\nMale or Female\n\n\nPredictor\n\n\n\n\nTenure\n\n\nNumeric\n\n\nMonths with provider\n\n\nPredictor\n\n\n\n\nMonthlyCharges\n\n\nNumeric\n\n\nMonthly fee\n\n\nPredictor\n\n\n\n\nContractType\n\n\nCategorical\n\n\nMonth to month, one year, two year\n\n\nPredictor\n\n\n\n\nInternetService\n\n\nCategorical\n\n\nDSL, fiber, or none\n\n\nPredictor\n\n\n\n\nTechSupport\n\n\nCategorical\n\n\nHas support yes or no\n\n\nPredictor\n\n\n\n\nTotalCharges\n\n\nNumeric\n\n\nTotal billed amount\n\n\nPredictor\n\n\n\n\nChurn\n\n\nTarget\n\n\nCustomer left yes or no\n\n\nTarget\n\n\n\n\n\n\nExploratory Data Analysis\nThis section summarizes data shape, missing fields, class balance, and key relationships that explain churn.\n\n\n1. Numeric features distribution\n\nAge Most customers are between thirty and fifty years old with an approximately normal shape.\n\nMonthlyCharges Values range from about twenty to one hundred twenty, reflecting different service tiers.\n\nTotalCharges Values range from zero to around twelve thousand, with most under two thousand. This positive skew suggests many newer or lower cost plans, with a smaller group of long term or premium users.\n\nTenure Spans zero to one hundred twenty months and is concentrated at the lower end, indicating many relatively new customers.\n\n\n\n\n\n\n\n2. Categorical features distribution\n\nContractType Month to month contracts dominate, suggesting a preference for flexibility.\nInternetService Fiber optic is most common.\n\nTechSupport With and without support are close to even.\n\nGender Female is slightly higher than male in this sample.\n\n\n\n\n\n\n\n3. Target variable distribution\n\nThe churn target is imbalanced, with more customers who churned than those who did not. This calls for attention to evaluation choices and possibly class weighting or threshold tuning.\n\n\n\n\n\n\n\n4. Correlation matrix and key relationships\n\nPositive Month to month contracts are strongly associated with higher churn Lack of tech support also aligns with higher churn.\nNegative One year and Two year contracts correlate with lower churn, indicating more loyalty and stability.\n\n\n\n\n\n\n\nTools\nModeling and data: Python (Pandas, scikit learn, matplotlib, seaborn)  Visualization and reporting: Power BI, Excel \n\n\nKey Methods and Approach\nThis study evaluated five supervised learning models to identify the most accurate method for predicting customer churn. The models include K Nearest Neighbors, Naive Bayes, Logistic Regression, Decision Tree, and Random Forest.\nGoal: Select a reliable predictive method for churn risk with clear business interpretation.\nData variants: Compare accuracy with original dataset without preprocessing and cleaned dataset with standard preprocessing.\nEvaluation:\n\nOverall accuracy\n\nPrecision, recall, and F1 score with special attention to the minority class labeled No\n\nPractical usefulness for targeted retention actions\n\n\nK Nearest NeighborsNaive BayesLogistic RegressionDecision TreeRandom Forest\n\n\nObjective\nCompare a K Nearest Neighbors classifier on the original dataset and on a cleaned dataset to see how preprocessing changes accuracy and class level metrics\nBenchmarks\n\nBenchmark 1 original dataset without preprocessing\n\nBenchmark 2 cleaned and preprocessed dataset\n\nEvaluation\n\nAccuracy\n\nPrecision, recall, and F1 with focus on the minority class labeled No\n\n\n\n\nObjective\nAssess Naive Bayes on three dataset variants to understand the impact of preprocessing and class balancing\nBenchmarks\n\nBenchmark 1 original dataset without preprocessing\n\nBenchmark 2 cleaned and preprocessed dataset\n\nBenchmark 3 cleaned dataset balanced with SMOTE\n\nEvaluation\n\nAccuracy\n\nPrecision, recall, and F1 for churned and non churned classes\n\nSensitivity to class imbalance\n\n\n\n\nObjective\nPredict churn using logistic regression and study the effect of preprocessing and threshold tuning on performance and recall\nBenchmarks\n\nBenchmark 1 model on the unprocessed dataset that includes missing values and unencoded categorical fields\n\nBenchmark 2 model after preprocessing that fills missing TotalCharges with the mean, scales numeric fields, and applies one hot encoding to categorical fields such as Gender and ContractType\n\nBenchmark 3 threshold optimization using the ROC curve, selecting a probability cutoff of 0.7 to balance precision and recall\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nBusiness friendly interpretation through model coefficients\n\n\n\n\nObjective\nUse a classification tree to predict churn and provide rules that are easy for partners to understand\nBenchmarks\n\nBenchmark 1 model on the original dataset without preprocessing\n\nBenchmark 2 model on the cleaned and preprocessed dataset\n\nBenchmark 3 model selection via cross validation to choose depth and split settings that generalize well\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nFeature importance to highlight drivers of churn\n\n\n\n\nObjective\nImprove generalization by averaging many trees and quantify gains from preprocessing and tuning\nBenchmarks\n\nBenchmark 1 model on the unprocessed dataset with missing values and unencoded categories\n\nBenchmark 2 model after preprocessing that imputes TotalCharges, standardizes numeric fields, and applies one hot encoding for categories\n\nBenchmark 3 hyperparameter search to find strong settings for number of trees, depth, and split criteria\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nFeature importance to highlight drivers of churn\n\n\n\n\n\n\nResults and Findings\n1. K Nearest Neighbors\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\n\n\n\n\nAccuracy\n\n\n0.84\n\n\n0.92\n\n\n\n\nPrecision for class No\n\n\n0.17\n\n\n0.60\n\n\n\n\nRecall for class No\n\n\n0.80\n\n\n0.94\n\n\n\n\nF1 for class No\n\n\n0.11\n\n\n0.74\n\n\n\n\nClass distribution\n\n\nYes: 39 No: 261\n\n\nYes: 39 No: 261\n\n\n\n\nFindings\n\nCleaning the data improved accuracy by eight percentage points\nOne hot encoding and binning reduced noise and helped the model learn clearer boundaries\nUpsampling addressed class imbalance and produced very large gains in recall and F1 for the minority class No\nThe original data led the model to favor the majority class Yes and missed many true No cases\nThe cleaned data produced a more balanced classifier that is useful for retention actions\n\nOverall KNN performs much better on the cleaned data. Preprocessing and class balancing were essential to identify at risk customers and to achieve balanced performance.\n2. Naive Bayes\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nSMOTE\n\n\n\n\n\n\nAccuracy\n\n\n0.832\n\n\n0.888\n\n\n0.804\n\n\n\n\nPrecision for class Yes\n\n\n0.97\n\n\n0.91\n\n\n0.96\n\n\n\n\nRecall for class Yes\n\n\n0.83\n\n\n0.96\n\n\n0.81\n\n\n\n\nF1 score\n\n\n0.90\n\n\n0.94\n\n\n0.88\n\n\n\n\nClass distribution\n\n\nYes: 27 No: 181\n\n\nYes: 13 No: 159\n\n\nYes: 33 No: 217\n\n\n\n\nFindings\n\nCleaning improved overall accuracy and raised recall for churn class Yes from eighty three percent to ninety six percent\nSMOTE balancing increased recall for the churn class relative to the original data but reduced overall accuracy to zero point eight zero four\nVery high precision on the original data reflects the class imbalance and a conservative decision boundary that missed some true churners\nThe cleaned dataset delivered the best balance of accuracy and class level metrics\nNext steps confirm robustness with stratified cross validation, review calibration, and tune the threshold to align recall with retention goals\n\n3. Logistic Regression\n\n\n\n\nBenchmark\n\n\nAccuracy\n\n\nPrecision Class 0\n\n\nRecall Class 0\n\n\nF1 score Class 0\n\n\nClass distribution\n\n\n\n\n\n\nWithout preprocessing\n\n\n0.94\n\n\n0.65\n\n\n0.42\n\n\n0.51\n\n\nYes: 88 No: 12\n\n\n\n\nWith preprocessing default\n\n\n0.95\n\n\n0.87\n\n\n0.79\n\n\n0.83\n\n\nYes: 87 No: 13\n\n\n\n\nWith preprocessing optimized threshold\n\n\n0.92\n\n\n0.63\n\n\n1.00\n\n\n0.78\n\n\nYes: 87 No: 13\n\n\n\n\nFindings\n\nCleaning and encoding improved accuracy from zero point ninety four to zero point ninety five and lifted recall for non churn class zero from forty two percent to seventy nine percent\nThreshold tuning increased recall for class zero to one hundred percent with a tradeoff in precision and overall accuracy to zero point ninety two\nRemoving missing values and encoding categories stabilized training and produced more reliable probabilities\nPreprocessed default settings give the best overall balance of accuracy and class level metrics, while the optimized threshold is useful when recall for non churn must be maximized for a specific business rule\n\n4. Decision Tree\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nCross-validation\n\n\n\n\n\n\nAccuracy\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nPrecision for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nRecall for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nF1 for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nClass distribution\n\n\nYes: 27 No: 223\n\n\nYes: 32 No: 144\n\n\nYes: 30 No: 150\n\n\n\n\nFindings\n\nAccuracy, precision, recall, and F1 were all one hundred percent on both original and cleaned data\nHeavy class imbalance with about eighty eight percent Yes likely inflated the metrics and reduced generalization\nPreprocessing did not change the winning features, but it shifted their relative importance\n\n5. Random Forest\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nHyperparameter tuning\n\n\n\n\n\n\nAccuracy\n\n\n0.96\n\n\n1.0\n\n\n0.996\n\n\n\n\nPrecision\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nRecall\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nF1 score\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nTrain score\n\n\n1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nTest score\n\n\n0.96\n\n\n1.0\n\n\n0.996\n\n\n\n\nClass distribution\n\n\nYes: 27No: 223\n\n\nYes: 33No: 217\n\n\nYes: 33No: 216\n\n\n\n\nFindings\n\nPreprocessing raised accuracy from zero point ninety six to one point zero and produced perfect precision, recall, and F1\nFilling missing TotalCharges and encoding categories reduced noise and likely helped the trees form cleaner splits\nClass balance shifted slightly toward Yes after cleaning, yet the target remains imbalanced near eighty six percent Yes\nPerfect metrics across train and test suggest limited stress on generalization; validate with stratified cross validation and report balanced accuracy and AUC\nHyperparameter tuning delivered accuracy of zero point nine nine six with other metrics unchanged, confirming a strong and stable model while still warranting checks for robustness on unseen data\n\n\n\nConclusion\nOur comparison shows that Decision Tree and Random Forest reached near perfect accuracy, while KNN and Logistic Regression also performed strongly at around the low to mid nineties. Naive Bayes trailed at about eighty eight percent on the cleaned data. Cross validation and confusion matrix checks did not indicate overfitting for the tree models, though the class imbalance means we should continue to validate with balanced accuracy and AUC on true holdouts.\nFor production use, pair one tree based model with one non tree model to balance strengths and reduce risk from unseen patterns. For example, deploy a Decision Tree or Random Forest alongside a KNN or Logistic Regression model, monitor both, and choose actions when they agree or when calibrated churn risk exceeds a threshold. With these models, a telecom company can flag at risk customers early and trigger targeted retention steps such as personalized discounts, proactive service outreach, and plan reviews to protect revenue and improve satisfaction.\n\n\nBusiness Implication\n\n\nRevenue and retention\n\nReduce churn through early outreach\nLift lifetime value with right sized plans\nShift spend from broad ads to focused saves\n\nOperations and service\n\nRoute high risk accounts to priority care\nTrigger proactive outreach after tickets or outages\nSet stronger service targets for high risk segments\n\n\nMarketing and offers\n\nPersonalize incentives by churn driver\nRun win back campaigns on persisting risk\nUse uplift testing to target customers who respond\n\nPlanning and governance\n\nForecast churn driven revenue and program ROI\nMonitor fairness by segment and keep decision logs\nIntegrate scores into CRM with clear reason codes\n\n\n\n\n\nNext Steps\nOur results are strong, but the target class is very imbalanced and Naive Bayes underperformed. This limits confidence in real world performance and suggests that the current metrics may be inflated by class skew. To help improve our models, we should consider the following steps:\n\nData and labeling\n\nCollect more recent records to reduce class skew and reflect current behavior\nBalance the target using resampling or class weights and compare to the current baseline\nAudit labels and remove duplicates that can inflate accuracy\n\nValidation and metrics\n\nUse stratified cross validation with a true holdout set\nAdd balanced accuracy, Matthews correlation, AUC ROC, and AUC PR\nUse time based validation if the data has a natural timeline\n\nModel robustness and calibration\n\nCalibrate probabilities for better threshold control\nTune thresholds by segment to meet business goals such as higher recall for churners\nFor Naive Bayes, add interaction features or consider a variant that handles continuous features more flexibly\nAdd a gradient boosted tree as an additional comparator\n\nFeature quality and leakage checks\n\nStandardize numeric fields and encode categorical fields consistently\nKeep binning only where it helps\nRecheck for target leakage and confirm drivers with permutation importance and SHAP\n\nMonitoring and operations\n\nTrack live KPIs such as monthly churn rate, save rate, net revenue saved, offer cost per save, and calibration error\nLog predictions with reason codes in the CRM so agents see top drivers\nSchedule retraining and drift checks on a regular cadence\n\nSummary action\n\nDeploy one tree model and one non tree model in parallel\nCalibrate and monitor both, then re evaluate once additional balanced data is available\n\n\n\n\nLinks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xinying Wu",
    "section": "",
    "text": "Hello! Welcome my portfolio website."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Xinying Wu",
    "section": "About Me",
    "text": "About Me\nMaster of Science in Business Analytics candidate at the University of California, Irvine’s Paul Merage School of Business with a strong foundation in Business Administration and Computer Information Science from the University of Oregon.\nI bring hands-on experience in data analytics, process optimization, and data-driven decision support. As a Business Analyst at Edwards Lifesciences, I developed Python-based automation models to detect anomalies in 190K+ regulatory distribution records, applied SQL and machine learning for pattern detection, and visualized insights through Tableau to enhance data quality oversight.\nMy academic projects include designing SQL-based financial dashboards to assess credit card fraud risk, predicting heart attack probability using R-based machine learning models, and modeling telecom customer churn with Python and Power BI.\nProficient in Python, R, SQL, Tableau, Power BI, and Excel, I excel at transforming raw data into actionable insights that improve business performance, strengthen compliance, and guide strategic decisions. Passionate about leveraging analytics to drive operational efficiency and innovation across organizations."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Xinying Wu",
    "section": "Education",
    "text": "Education\nUniversity of California, Irvine | Irvine, CA\nM.S. in Business Analytics | Jul. 2024 - Jun. 2025\nThe University of Oregon | Eugene, OR\nB.S. in Business Administration | Sep. 2018 - Jun. 2022\nMinor in Computer Information Science | Sep. 2018 - Jun. 2022"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Xinying Wu",
    "section": "Experience",
    "text": "Experience\nEdwards Lifesciences, Irvine, CA | Business Analyst | Jan. 2025 - Jun. 2025\n\n• Built Python automation models and applied unsupervised machine learning with SQL to detect anomalies in 190K+ regulatory distribution records, improving data integrity and supply chain quality oversight.\n• Created Tableau and Excel dashboards to visualize error-prone categories, conduct gap analysis, and deliver actionable insights that guided operational improvements.\n• Contributed to data governance documentation and collaborated with Edwards stakeholders through weekly reviews to refine strategies aligned with compliance and business goals.\n\nGEO.S.BUSH & CO, Portland, OR | International Trade Representative | Oct. 2022 - Apr. 2024\n\n• Managed end-to-end logistics documentation to ensure timely customs clearance, regulatory compliance, and smooth operational workflows.\n• Audited shipment data against purchase orders and submitted compliance documents to maintain data accuracy, support integrity, and minimize delivery delays.\n• Collaborated with internal teams and external partners to resolve shipment discrepancies and streamline processes, demonstrating strong organizational and problem-solving skills."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Job Recommendation System\n\n\n\nPython\n\nNLP\n\nRecommendation\n\n\n\nBuilt a skill-based job recommendation system using NLP and cosine similarity on 1.3M+ LinkedIn job postings. Delivered a working prototype with a real-time matching…\n\n\n\nMar 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Churn Prediction\n\n\n\nPython\n\nPower BI\n\nClassification\n\n\n\nCleaned telecom data, engineered features, trained a classifier, and built visuals to guide retention strategy.\n\n\n\nDec 10, 2024\n\n\n\n\n\n\nNo matching items"
  }
]
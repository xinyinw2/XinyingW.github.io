[
  {
    "objectID": "projects/cust_churn.html",
    "href": "projects/cust_churn.html",
    "title": "Customer Churn Prediction",
    "section": "",
    "text": "Overview\nThis project analyzes customer attrition in the telecom sector using the dataset from Kaggle titled “Customer Churn Prediction:Analysis”. The goal is to discover the factors that most strongly relate to attrition and to build prediction models that flag accounts with high risk. The work translates analytic results into actions that support retention planning, resource allocation, and service improvements. Deliverables include clear explanations of churn drivers, prediction outputs for at-risk customers, and visuals that nontechnical partners can use in planning.\n\n\nProblem Statement\nTelecom firms face revenue loss and higher acquisition cost when customers discontinue service. Without timely prediction, teams respond after the customer has already left, which reduces the effect of retention efforts and can damage brand reputation. The objective is to create an analytic workflow that\n\nidentifies the variables that explain attrition with clarity for business partners\npredicts the probability that each active account will discontinue service in the next period\nprovides ranked customer lists with reason codes so teams can act with targeted offers and service interventions\n\n\n\nData\nThis project uses a publicly available telecom customer churn dataset sourced from Kaggle. The dataset provides a comprehensive view of customer behavior and churn in the telecom industry, including customer demographics, service usage, contract attributes, billing amounts, and the churn outcome.\nSnapshot\n\nRecords: 1,000 customers\nFeatures: 10 columns\nTarget: Churn yes or no\n\nPreprocessing summary\n\nKept Churn as the dependent variable\nDropped CustomerID since it carries no predictive signal\nConverted categorical fields to dummy variables for modeling compatibility\nVerified that TotalCharges is approximately MonthlyCharges multiplied by Tenure, with any data quality issues handled during cleaning\n\nFeature dictionary\n\n\n\n\nFeature\n\n\nType\n\n\nDescription\n\n\nRole\n\n\n\n\n\n\nCustomerID\n\n\nID\n\n\nUnique customer key\n\n\nExcluded\n\n\n\n\nAge\n\n\nNumeric\n\n\nCustomer age in years\n\n\nPredictor\n\n\n\n\nGender\n\n\nCategorical\n\n\nMale or Female\n\n\nPredictor\n\n\n\n\nTenure\n\n\nNumeric\n\n\nMonths with provider\n\n\nPredictor\n\n\n\n\nMonthlyCharges\n\n\nNumeric\n\n\nMonthly fee\n\n\nPredictor\n\n\n\n\nContractType\n\n\nCategorical\n\n\nMonth to month, one year, two year\n\n\nPredictor\n\n\n\n\nInternetService\n\n\nCategorical\n\n\nDSL, fiber, or none\n\n\nPredictor\n\n\n\n\nTechSupport\n\n\nCategorical\n\n\nHas support yes or no\n\n\nPredictor\n\n\n\n\nTotalCharges\n\n\nNumeric\n\n\nTotal billed amount\n\n\nPredictor\n\n\n\n\nChurn\n\n\nTarget\n\n\nCustomer left yes or no\n\n\nTarget\n\n\n\n\n\n\nExploratory Data Analysis\nThis section summarizes data shape, missing fields, class balance, and key relationships that explain churn.\n\n\n1. Numeric features distribution\n\nAge Most customers are between thirty and fifty years old with an approximately normal shape.\n\nMonthlyCharges Values range from about twenty to one hundred twenty, reflecting different service tiers.\n\nTotalCharges Values range from zero to around twelve thousand, with most under two thousand. This positive skew suggests many newer or lower cost plans, with a smaller group of long term or premium users.\n\nTenure Spans zero to one hundred twenty months and is concentrated at the lower end, indicating many relatively new customers.\n\n\n\n\n\n\n\n2. Categorical features distribution\n\nContractType Month to month contracts dominate, suggesting a preference for flexibility.\nInternetService Fiber optic is most common.\n\nTechSupport With and without support are close to even.\n\nGender Female is slightly higher than male in this sample.\n\n\n\n\n\n\n\n3. Target variable distribution\n\nThe churn target is imbalanced, with more customers who churned than those who did not. This calls for attention to evaluation choices and possibly class weighting or threshold tuning.\n\n\n\n\n\n\n\n4. Correlation matrix and key relationships\n\nPositive Month to month contracts are strongly associated with higher churn Lack of tech support also aligns with higher churn.\nNegative One year and Two year contracts correlate with lower churn, indicating more loyalty and stability.\n\n\n\n\n\n\n\nTools\nModeling and data: Python (Pandas, scikit learn, matplotlib, seaborn)  Visualization and reporting: Power BI, Excel \n\n\nKey Methods and Approach\nThis study evaluated five supervised learning models to identify the most accurate method for predicting customer churn. The models include K Nearest Neighbors, Naive Bayes, Logistic Regression, Decision Tree, and Random Forest.\nGoal: Select a reliable predictive method for churn risk with clear business interpretation.\nData variants: Compare accuracy with original dataset without preprocessing and cleaned dataset with standard preprocessing.\nEvaluation:\n\nOverall accuracy\n\nPrecision, recall, and F1 score with special attention to the minority class labeled No\n\nPractical usefulness for targeted retention actions\n\n\nK Nearest NeighborsNaive BayesLogistic RegressionDecision TreeRandom Forest\n\n\nObjective\nCompare a K Nearest Neighbors classifier on the original dataset and on a cleaned dataset to see how preprocessing changes accuracy and class level metrics\nBenchmarks\n\nBenchmark 1 original dataset without preprocessing\n\nBenchmark 2 cleaned and preprocessed dataset\n\nEvaluation\n\nAccuracy\n\nPrecision, recall, and F1 with focus on the minority class labeled No\n\n\n\n\nObjective\nAssess Naive Bayes on three dataset variants to understand the impact of preprocessing and class balancing\nBenchmarks\n\nBenchmark 1 original dataset without preprocessing\n\nBenchmark 2 cleaned and preprocessed dataset\n\nBenchmark 3 cleaned dataset balanced with SMOTE\n\nEvaluation\n\nAccuracy\n\nPrecision, recall, and F1 for churned and non churned classes\n\nSensitivity to class imbalance\n\n\n\n\nObjective\nPredict churn using logistic regression and study the effect of preprocessing and threshold tuning on performance and recall\nBenchmarks\n\nBenchmark 1 model on the unprocessed dataset that includes missing values and unencoded categorical fields\n\nBenchmark 2 model after preprocessing that fills missing TotalCharges with the mean, scales numeric fields, and applies one hot encoding to categorical fields such as Gender and ContractType\n\nBenchmark 3 threshold optimization using the ROC curve, selecting a probability cutoff of 0.7 to balance precision and recall\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nBusiness friendly interpretation through model coefficients\n\n\n\n\nObjective\nUse a classification tree to predict churn and provide rules that are easy for partners to understand\nBenchmarks\n\nBenchmark 1 model on the original dataset without preprocessing\n\nBenchmark 2 model on the cleaned and preprocessed dataset\n\nBenchmark 3 model selection via cross validation to choose depth and split settings that generalize well\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nFeature importance to highlight drivers of churn\n\n\n\n\nObjective\nImprove generalization by averaging many trees and quantify gains from preprocessing and tuning\nBenchmarks\n\nBenchmark 1 model on the unprocessed dataset with missing values and unencoded categories\n\nBenchmark 2 model after preprocessing that imputes TotalCharges, standardizes numeric fields, and applies one hot encoding for categories\n\nBenchmark 3 hyperparameter search to find strong settings for number of trees, depth, and split criteria\n\nEvaluation\n\nAccuracy and AUC\n\nClass level precision, recall, and F1\n\nFeature importance to highlight drivers of churn\n\n\n\n\n\n\nResults and Findings\n1. K Nearest Neighbors\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\n\n\n\n\nAccuracy\n\n\n0.84\n\n\n0.92\n\n\n\n\nPrecision for class No\n\n\n0.17\n\n\n0.60\n\n\n\n\nRecall for class No\n\n\n0.80\n\n\n0.94\n\n\n\n\nF1 for class No\n\n\n0.11\n\n\n0.74\n\n\n\n\nClass distribution\n\n\nYes: 39 No: 261\n\n\nYes: 39 No: 261\n\n\n\n\nFindings\n\nCleaning the data improved accuracy by eight percentage points\nOne hot encoding and binning reduced noise and helped the model learn clearer boundaries\nUpsampling addressed class imbalance and produced very large gains in recall and F1 for the minority class No\nThe original data led the model to favor the majority class Yes and missed many true No cases\nThe cleaned data produced a more balanced classifier that is useful for retention actions\n\nOverall KNN performs much better on the cleaned data. Preprocessing and class balancing were essential to identify at risk customers and to achieve balanced performance.\n2. Naive Bayes\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nSMOTE\n\n\n\n\n\n\nAccuracy\n\n\n0.832\n\n\n0.888\n\n\n0.804\n\n\n\n\nPrecision for class Yes\n\n\n0.97\n\n\n0.91\n\n\n0.96\n\n\n\n\nRecall for class Yes\n\n\n0.83\n\n\n0.96\n\n\n0.81\n\n\n\n\nF1 score\n\n\n0.90\n\n\n0.94\n\n\n0.88\n\n\n\n\nClass distribution\n\n\nYes: 27 No: 181\n\n\nYes: 13 No: 159\n\n\nYes: 33 No: 217\n\n\n\n\nFindings\n\nCleaning improved overall accuracy and raised recall for churn class Yes from eighty three percent to ninety six percent\nSMOTE balancing increased recall for the churn class relative to the original data but reduced overall accuracy to zero point eight zero four\nVery high precision on the original data reflects the class imbalance and a conservative decision boundary that missed some true churners\nThe cleaned dataset delivered the best balance of accuracy and class level metrics\nNext steps confirm robustness with stratified cross validation, review calibration, and tune the threshold to align recall with retention goals\n\n3. Logistic Regression\n\n\n\n\nBenchmark\n\n\nAccuracy\n\n\nPrecision Class 0\n\n\nRecall Class 0\n\n\nF1 score Class 0\n\n\nClass distribution\n\n\n\n\n\n\nWithout preprocessing\n\n\n0.94\n\n\n0.65\n\n\n0.42\n\n\n0.51\n\n\nYes: 88 No: 12\n\n\n\n\nWith preprocessing default\n\n\n0.95\n\n\n0.87\n\n\n0.79\n\n\n0.83\n\n\nYes: 87 No: 13\n\n\n\n\nWith preprocessing optimized threshold\n\n\n0.92\n\n\n0.63\n\n\n1.00\n\n\n0.78\n\n\nYes: 87 No: 13\n\n\n\n\nFindings\n\nCleaning and encoding improved accuracy from zero point ninety four to zero point ninety five and lifted recall for non churn class zero from forty two percent to seventy nine percent\nThreshold tuning increased recall for class zero to one hundred percent with a tradeoff in precision and overall accuracy to zero point ninety two\nRemoving missing values and encoding categories stabilized training and produced more reliable probabilities\nPreprocessed default settings give the best overall balance of accuracy and class level metrics, while the optimized threshold is useful when recall for non churn must be maximized for a specific business rule\n\n4. Decision Tree\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nCross-validation\n\n\n\n\n\n\nAccuracy\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nPrecision for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nRecall for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nF1 for class No\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nClass distribution\n\n\nYes: 27 No: 223\n\n\nYes: 32 No: 144\n\n\nYes: 30 No: 150\n\n\n\n\nFindings\n\nAccuracy, precision, recall, and F1 were all one hundred percent on both original and cleaned data\nHeavy class imbalance with about eighty eight percent Yes likely inflated the metrics and reduced generalization\nPreprocessing did not change the winning features, but it shifted their relative importance\n\n5. Random Forest\n\n\n\n\nMetric\n\n\nOriginal dataset\n\n\nCleaned dataset\n\n\nHyperparameter tuning\n\n\n\n\n\n\nAccuracy\n\n\n0.96\n\n\n1.0\n\n\n0.996\n\n\n\n\nPrecision\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nRecall\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nF1 score\n\n\n~1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nTrain score\n\n\n1.0\n\n\n1.0\n\n\n1.0\n\n\n\n\nTest score\n\n\n0.96\n\n\n1.0\n\n\n0.996\n\n\n\n\nClass distribution\n\n\nYes: 27No: 223\n\n\nYes: 33No: 217\n\n\nYes: 33No: 216\n\n\n\n\nFindings\n\nPreprocessing raised accuracy from zero point ninety six to one point zero and produced perfect precision, recall, and F1\nFilling missing TotalCharges and encoding categories reduced noise and likely helped the trees form cleaner splits\nClass balance shifted slightly toward Yes after cleaning, yet the target remains imbalanced near eighty six percent Yes\nPerfect metrics across train and test suggest limited stress on generalization; validate with stratified cross validation and report balanced accuracy and AUC\nHyperparameter tuning delivered accuracy of zero point nine nine six with other metrics unchanged, confirming a strong and stable model while still warranting checks for robustness on unseen data\n\n\n\nConclusion\nOur comparison shows that Decision Tree and Random Forest reached near perfect accuracy, while KNN and Logistic Regression also performed strongly at around the low to mid nineties. Naive Bayes trailed at about eighty eight percent on the cleaned data. Cross validation and confusion matrix checks did not indicate overfitting for the tree models, though the class imbalance means we should continue to validate with balanced accuracy and AUC on true holdouts.\nFor production use, pair one tree based model with one non tree model to balance strengths and reduce risk from unseen patterns. For example, deploy a Decision Tree or Random Forest alongside a KNN or Logistic Regression model, monitor both, and choose actions when they agree or when calibrated churn risk exceeds a threshold. With these models, a telecom company can flag at risk customers early and trigger targeted retention steps such as personalized discounts, proactive service outreach, and plan reviews to protect revenue and improve satisfaction.\n\n\nBusiness Implication\n\n\nRevenue and retention\n\nReduce churn through early outreach\nLift lifetime value with right sized plans\nShift spend from broad ads to focused saves\n\nOperations and service\n\nRoute high risk accounts to priority care\nTrigger proactive outreach after tickets or outages\nSet stronger service targets for high risk segments\n\n\nMarketing and offers\n\nPersonalize incentives by churn driver\nRun win back campaigns on persisting risk\nUse uplift testing to target customers who respond\n\nPlanning and governance\n\nForecast churn driven revenue and program ROI\nMonitor fairness by segment and keep decision logs\nIntegrate scores into CRM with clear reason codes\n\n\n\n\n\nNext Steps\nOur results are strong, but the target class is very imbalanced and Naive Bayes underperformed. This limits confidence in real world performance and suggests that the current metrics may be inflated by class skew. To help improve our models, we should consider the following steps:\n\nData and labeling\n\nCollect more recent records to reduce class skew and reflect current behavior\nBalance the target using resampling or class weights and compare to the current baseline\nAudit labels and remove duplicates that can inflate accuracy\n\nValidation and metrics\n\nUse stratified cross validation with a true holdout set\nAdd balanced accuracy, Matthews correlation, AUC ROC, and AUC PR\nUse time based validation if the data has a natural timeline\n\nModel robustness and calibration\n\nCalibrate probabilities for better threshold control\nTune thresholds by segment to meet business goals such as higher recall for churners\nFor Naive Bayes, add interaction features or consider a variant that handles continuous features more flexibly\nAdd a gradient boosted tree as an additional comparator\n\nFeature quality and leakage checks\n\nStandardize numeric fields and encode categorical fields consistently\nKeep binning only where it helps\nRecheck for target leakage and confirm drivers with permutation importance and SHAP\n\nMonitoring and operations\n\nTrack live KPIs such as monthly churn rate, save rate, net revenue saved, offer cost per save, and calibration error\nLog predictions with reason codes in the CRM so agents see top drivers\nSchedule retraining and drift checks on a regular cadence\n\nSummary action\n\nDeploy one tree model and one non tree model in parallel\nCalibrate and monitor both, then re evaluate once additional balanced data is available\n\n\n\n\nLinks"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Job Recommendation System\n\n\n\nPython\n\nNLP\n\nRecommendation\n\n\n\nMatches candidates to roles using a large LinkedIn Jobs and Skills dataset, with skill based similarity and TF-IDF methods to return the most relevant openings.\n\n\n\nMar 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeart Attack Prediction\n\n\n\nR\n\nTableau\n\nClassification\n\n\n\nPredicts elevated heart-attack risk using machine learning and explains key drivers to support prevention and clinical decisions.\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Churn Prediction\n\n\n\nPython\n\nPower BI\n\nClassification\n\n\n\nPredicts which telecom customers are most likely to leave and explains the key drivers, using supervised models and clear visuals so teams can act on retention quickly.\n\n\n\nDec 10, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xinying Wu",
    "section": "",
    "text": "Hello! Welcome my portfolio website."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Xinying Wu",
    "section": "About Me",
    "text": "About Me\nMaster of Science in Business Analytics candidate at the University of California, Irvine’s Paul Merage School of Business with a strong foundation in Business Administration and Computer Information Science from the University of Oregon.\nI bring hands-on experience in data analytics, process optimization, and data-driven decision support. As a Business Analyst at Edwards Lifesciences, I developed Python-based automation models to detect anomalies in 190K+ regulatory distribution records, applied SQL and machine learning for pattern detection, and visualized insights through Tableau to enhance data quality oversight.\nMy academic projects include designing SQL-based financial dashboards to assess credit card fraud risk, predicting heart attack probability using R-based machine learning models, and modeling telecom customer churn with Python and Power BI.\nProficient in Python, R, SQL, Tableau, Power BI, and Excel, I excel at transforming raw data into actionable insights that improve business performance, strengthen compliance, and guide strategic decisions. Passionate about leveraging analytics to drive operational efficiency and innovation across organizations."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Xinying Wu",
    "section": "Education",
    "text": "Education\nUniversity of California, Irvine | Irvine, CA\nM.S. in Business Analytics | Jul. 2024 - Jun. 2025\nThe University of Oregon | Eugene, OR\nB.S. in Business Administration | Sep. 2018 - Jun. 2022\nMinor in Computer Information Science | Sep. 2018 - Jun. 2022"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Xinying Wu",
    "section": "Experience",
    "text": "Experience\nEdwards Lifesciences, Irvine, CA | Business Analyst | Jan. 2025 - Jun. 2025\n\n• Built Python automation models and applied unsupervised machine learning with SQL to detect anomalies in 190K+ regulatory distribution records, improving data integrity and supply chain quality oversight.\n• Created Tableau and Excel dashboards to visualize error-prone categories, conduct gap analysis, and deliver actionable insights that guided operational improvements.\n• Contributed to data governance documentation and collaborated with Edwards stakeholders through weekly reviews to refine strategies aligned with compliance and business goals.\n\nGEO.S.BUSH & CO, Portland, OR | International Trade Representative | Oct. 2022 - Apr. 2024\n\n• Managed end-to-end logistics documentation to ensure timely customs clearance, regulatory compliance, and smooth operational workflows.\n• Audited shipment data against purchase orders and submitted compliance documents to maintain data accuracy, support integrity, and minimize delivery delays.\n• Collaborated with internal teams and external partners to resolve shipment discrepancies and streamline processes, demonstrating strong organizational and problem-solving skills."
  },
  {
    "objectID": "projects/heart.html",
    "href": "projects/heart.html",
    "title": "Heart Attack Prediction",
    "section": "",
    "text": "Overview\nThis project develops a predictive model to flag individuals at elevated risk of heart attack, combining machine learning with interpretable analytics so results are useful for screening, prevention, and clinical decision support. We focus on identifying which clinical, lifestyle, and demographic factors most influence risk, and on calibrating the model’s precision and recall to be reliable for real-world use.\nWhy this matters\n\nHeart disease is a leading cause of death worldwide (18M+ deaths annually).\n\nIn the U.S., someone has a heart attack every ~40 seconds, and over 50% report no prior symptoms.\n\nThe economic burden exceeds $200B annually.\n\nWho benefits\n\nPatients: personalized risk scores and driver explanations to guide lifestyle changes, medication adherence, and timely medical follow-up.\n\nClinicians: an evidence-based complement to clinical judgment that helps prioritize care and manage panels efficiently.\n\nPolicymakers & health systems: population-level insights to target prevention programs and reduce cardiovascular burden.\n\nResearch questions\n\nWhich factors most strongly influence heart-attack risk in our data?\n\nWhat precision/recall balance makes the model dependable for screening and referral?\n\nHow can insights translate into preventive actions for patients (e.g., lifestyle changes, adherence)?\n\n\n\nProblem statement\nCardiovascular disease is a leading cause of death, and many heart attacks occur without prior symptoms. Screening resources are limited, and risk is often underestimated until it is too late. Patients need clear, interpretable feedback they can act on, and clinicians need a reliable way to identify elevated risk earlier and prioritize preventive care. The core problem is to build a model that estimates individual heart-attack risk from routinely available data and explains why a person is high risk in time to intervene.\nOur objectives are to predict near-term heart-attack risk at the individual level, explain the top risk drivers in language patients and clinicians understand, and calibrate precision and recall so the model is safe for screening (high recall) and practical for follow-up (adequate precision). The scope includes clinical, lifestyle, and demographic inputs commonly captured in care or surveys, and concise contributing factors. We must handle class imbalance and missing data while maintaining interpretability and fairness across subgroups.\nSuccess will be defined by meeting agreed performance thresholds (e.g., strong AUROC and PR-AUC) and an operational precision/recall target set with clinical stakeholders. We will also look for stable performance across age and sex subgroups, documented bias checks, and explanation quality that clinicians rate as useful for counseling and care planning.\n\n\nData\nThe dataset comes from a dataset named “Heart Attack Risk Prediction Dataset” from Kaggle. This dataset is designed to analyze and predict heart attack risk based on various health, lifestyle, and demographic factors. It includes attributes such as age, cholesterol levels, blood pressure, smoking habits, exercise patterns, dietary preferences, and medical history. By leveraging predictive analytics and machine learning, this dataset can aid researchers and healthcare professionals in developing proactive strategies for heart disease prevention and management. .\n\nSnapshot\n\nRows: 8,763\nColumns: 26\nTarget variable: Heart_Attack_Risk (binary: 1 = At Risk, 0 = No Risk)\nPredictors: the modeling dataset includes 24 features across demographics, clinical/physiologic measures, medical history, and lifestyle—with Patient ID excluded from the final set\n\n\n\nData Dictionary\n\n\n\n\nFeature\n\n\nType\n\n\nDescription\n\n\n\n\n\n\nPatient ID\n\n\nQL(Unique)\n\n\nUnique identifier for each patient\n\n\n\n\nAge\n\n\nQ(Int)\n\n\nAge in years\n\n\n\n\nSex\n\n\nQL(2)\n\n\nMale or Female\n\n\n\n\nCholesterol\n\n\nQ(Int)\n\n\nTotal cholesterol (mg/dL)\n\n\n\n\nBlood Pressure\n\n\nQL(2)\n\n\nSystolic / Diastolic (mmHg)\n\n\n\n\nHeart Rate\n\n\nQ(Int)\n\n\nResting heart rate (bpm)\n\n\n\n\nDiabetes\n\n\nQL(Bin)\n\n\nDiabetes status (Yes/No)\n\n\n\n\nFamily History\n\n\nQL(Bin)\n\n\nFamily history of heart problems (1 Yes / 0 No)\n\n\n\n\nSmoking\n\n\nQL(Bin)\n\n\nSmoking status (1 Smoker / 0 Non-smoker)\n\n\n\n\nObesity\n\n\nQL(Bin)\n\n\nObesity status (1 Obese / 0 Not obese)\n\n\n\n\nAlcohol Consumption\n\n\nQL(4)\n\n\nNone / Light / Moderate / Heavy\n\n\n\n\nExercise Hours Per Week\n\n\nQ(Real)\n\n\nHours of exercise per week\n\n\n\n\nDiet\n\n\nQL(3)\n\n\nDiet quality (Healthy / Average / Unhealthy)\n\n\n\n\nPrevious Heart Problems\n\n\nQL(Bin)\n\n\nPrior heart-related issues (1 Yes / 0 No)\n\n\n\n\nMedication Use\n\n\nQL(Bin)\n\n\nTakes medication (1 Yes / 0 No)\n\n\n\n\nStress Level\n\n\nQ(Int)\n\n\nSelf-reported stress (1–10)\n\n\n\n\nSedentary Hours Per Day\n\n\nQ(Real)\n\n\nAverage sedentary hours per day\n\n\n\n\nIncome\n\n\nQ(Real)\n\n\nIncome level\n\n\n\n\nBMI\n\n\nQ(Real)\n\n\nBody Mass Index\n\n\n\n\nTriglycerides\n\n\nQ(Int)\n\n\nTriglycerides (mg/dL)\n\n\n\n\nPhysical Activity Days Per Week\n\n\nQ(Int)\n\n\nDays with physical activity (per week)\n\n\n\n\nSleep Hours Per Day\n\n\nQ(Real)\n\n\nAverage sleep hours per day\n\n\n\n\nCountry\n\n\nQL(N)\n\n\nCountry of residence\n\n\n\n\nContinent\n\n\nQL(7)\n\n\nContinent of residence\n\n\n\n\nHemisphere\n\n\nQL(2)\n\n\nNorthern / Southern\n\n\n\n\nHeart Attack Risk\n\n\nQL(Bin)\n\n\nTarget label (1 Yes / 0 No)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotation used\nQ(Int) = Quantitative (Integer)\nQ(Real) = Quantitative (Real)\nQL(Bin) = Qualitative (Binary)\nQL(N) = Qualitative (Categorical) with N levels\n\n\n\n\nData Preprocessing\n\nColumns\n\nDrop Patient.ID\nMove Heart.Attack.Risk to the first column\n\nMissing values and outliers\n\nChecked with a z-score screen\n\nNone detected (no imputation or trimming applied)\n\nBlood pressure parsing\n\nSplit Blood.Pressure into Systolic and Diastolic\n\nExample: 158/88 → Systolic = 158, Diastolic = 88\n\nEncode categorical variables\n\nBinary to 0/1\n\nSex → Female (1 = Female, 0 = Male)\nHemisphere → Northern.Hemisphere (1 = Northern, 0 = Southern)\nOthers were kept in numeric form when building a correlation matrix before being converted to factors for training models\n\nMulti-category (one-hot dummies)\n\nCreated dummies for Diet, Country, Continent\nConverted to numeric first when building a correlation matrix\nConverted to factors and finally dummies for training models\n\n\nScale numeric features\n\nStandardize all numeric predictors to mean = 0 and standard deviation = 1\n\n\n\n\nDataset versions and splits\nTwo final clean datasets (same row split, different handling of multi-category variables)\n\ndata: 26 columns, multi-category variables kept as factors\n\ndata1: 52 columns, multi-category variables dummy-encoded\n\n\n\n\n\nDataset\n\n\nColumns\n\n\nTotal rows\n\n\nTrain rows\n\n\nTest rows\n\n\n\n\n\n\ndata\n\n\n26\n\n\n8,763\n\n\n3,138\n\n\n4,382\n\n\n\n\ndata1\n\n\n52\n\n\n8,763\n\n\n3,138\n\n\n4,382\n\n\n\n\nWhich dataset for which models\n\nNumeric-only algorithms: use data1 (dummy-encoded) — e.g., k-NN, SVM\nModels that accept factors in our toolchain: use data (factor-based) — e.g., Logistic Regression, Naive Bayes, Decision Tree, Random Forest (Using factors also yields clearer tree visualizations.)\n\nClass distribution strategy\n\nOriginal distribution: ~36% “At risk”, ~64% “No risk”\nTest set (stratified): kept the original distribution (~36/64) to reflect real world prevalence\nTraining set (balanced): adjusted to 50% / 50% (“At risk” / “No risk”) to reduce class-imbalance bias during learning\n\nNotes\n\nBoth datasets share the same train/test split, ensuring fair model comparisons.\n“Balanced” training was achieved by resampling so each class contributes equally.\n\n\n\n\nExploratory Data Analysis\n\nDescriptive Analysis\n\nHeart Attack Risk by Country\n\n\nThis chart shows the proportion of people labeled At Risk vs No Risk for heart attack in each country.\nThe overall pattern is very similar across countries: most countries have an at-risk share of roughly one third.\nThis suggests that risk is widespread and not driven by a single country in the dataset.\n\n\n\n\n\n\n\nDistribution of At-Risk Patients by Age\n\n\nThis plot shows how many At Risk cases appear at different ages.\nRisk is not just concentrated in older adults — there are many at-risk individuals starting in their mid 20s and continuing through the 60s, 70s, and 80s.\nThat means elevated risk appears across the full adult age range.\n\n\n\n\n\n\n\nHeart Attack Risk by Gender\n\n\nThe share of people labeled At Risk is very similar between males and females.\nIn this dataset, baseline heart attack risk looks fairly balanced across gender, with no large gap in proportions.\n\n\n\n\n\n\n\nCorrelation Analysis\n\nCorrelation with Demographics\n\n\nThis correlation matrix looks at how Heart.Attack.Risk relates to demographic features such as Sex, Age, Income, Country, Continent, and Hemisphere.\nAll the correlation values with Heart.Attack.Risk are very close to zero, which means there is no strong linear relationship between risk and any single demographic or location variable in this dataset. This suggests that heart attack risk is not being driven by demographic alone, and likely depends more on clinical and lifestyle factors than on where someone lives.\n\n\n\n\n\n\n\nCorrelation with Physiological and Clinical Variables\n\n\nThis correlation matrix shows how Heart.Attack.Risk relates to physiological and clinical measures such as Cholesterol, Systolic and Diastolic blood pressure, Triglycerides, Heart.Rate, and BMI.\nThe strongest positive correlations appear between Cholesterol and risk, and between Systolic blood pressure and risk.\nThis suggests that higher cholesterol levels and higher systolic pressure are more associated with being labeled “At Risk” in this dataset, while the other variables have weaker or near-zero relationships.\n\n\n\n\n\n\n\nCorrelation with Medical History\n\n\nThis correlation matrix looks at how Heart.Attack.Risk relates to medical history factors such as Diabetes, Family.History, Previous.Heart.Problems, Medication.Use, and Stress.Level.\nThe strongest positive correlation with being “At Risk” is Diabetes, suggesting that a history of diabetes is an important signal for elevated heart attack risk in this dataset.\nOther history variables show weaker or near-zero direct correlations.\n\n\n\n\n\n\n\nCorrelation with Lifestyle Factors\n\n\nThis correlation matrix shows how Heart.Attack.Risk relates to lifestyle factors such as Smoking, Alcohol.Consumption, Diet, Exercise.Hours.Per.Week, Physical.Activity.Days.Per.Week, Sedentary.Hours.Per.Day, and Sleep.Hours.Per.Day.\nThe strongest relationship we see is a negative correlation between Sleep.Hours.Per.Day and heart attack risk, suggesting that fewer hours of sleep per day is associated with higher risk.\nOther lifestyle variables, including smoking and alcohol use, have weaker direct correlations in this dataset.\n\n\n\n\n\n\n\n\nTools\nModeling and data: R  Visualization and reporting: Tableau \n\n\nMethods and Approach\nTo predict heart attack risk, we trained and compared six different classification models.\nOur goal was to identify the model that performs best on unseen data while staying clinically interpretable.\nWe evaluated each model primarily on test accuracy, using the same train/test split strategy described above.\n\nModels evaluated\n1. Logistic Regression\nA linear, probability-based classifier. It estimates the chance that a patient is “At Risk” given their features.\nStrengths: simple, interpretable, easy to explain to clinicians.\nLimitation: assumes a mostly linear relationship between predictors and risk.\n2. Naive Bayes\nA probabilistic classifier that uses Bayes’ rule to estimate how likely each class is, given the input features. It assumes the features are conditionally independent once you know the class, which keeps the math very simple and fast.\nStrengths: works well on high-dimensional data, easy to train, very fast, and often surprisingly strong even with limited data.\nLimitation: the independence assumption is usually not true in real data, so performance can drop when features are strongly correlated.\n3. k-Nearest Neighbors (k-NN)\nA distance-based model. For a new patient, it looks at the most similar patients in the training set and votes.\nStrengths: non-parametric, can capture local patterns.\nLimitations: can be memory-intensive and sensitive to scaling; performance drops in high dimensions if features are noisy.\n4. Decision Tree\nA rule-based model that splits patients into groups using yes/no questions (for example, “Is systolic &gt; X?”).\nStrengths: highly interpretable, mirrors clinical reasoning.\nLimitation: can overfit if not pruned.\n5. Random Forest\nAn ensemble of many decision trees that vote together.\nStrengths: higher accuracy and more stability than a single tree; reduces overfitting.\nLimitation: less interpretable than a single tree, but still allows feature importance analysis.\n6. Support Vector Machine (SVM)\nFinds the best separating boundary (hyperplane) between “At Risk” and “No Risk,” and can use kernels to handle non-linear boundaries.\nStrengths: strong performance in high-dimensional feature spaces.\nLimitation: less intuitive to explain clinically compared to logistic regression or trees.\n\n\nModel selection\nEach model was trained on the balanced training set and tested on the realistic test set (with the original class ratio).\nWe compared their test accuracy side by side to understand which approach generalizes best.\nThat final comparison guides which model is most promising for deployment and clinical interpretation.\n\n\n\nModel results and selection\n\nTrained on Worldwide Data\nWe trained all six models (Logistic Regression, Naive Bayes, k-NN, Decision Tree, Random Forest, and SVM) on our processed training data and then evaluated them on the held-out test set.\nOur main comparison metric was test accuracy, using the same train/test split strategy described earlier.\n\n\n\n\nModel\n\n\nTest Accuracy\n\n\n\n\n\n\nLogistic Regression\n\n\n55%\n\n\n\n\nNaive Bayes\n\n\n50%\n\n\n\n\nk-NN\n\n\n51.67%\n\n\n\n\nDecision Tree\n\n\n51.32%\n\n\n\n\nRandom Forest\n\n\n51.37%\n\n\n\n\nSVM (Support Vector Machine)\n\n\n64.23%\n\n\n\n\n\n1. Logistic Regression\nHow we built the model\n\nStart with all features\nWe first fit a full logistic regression model on data.train using every available predictor.\nSelect the most important predictors\nWe ran stepwise feature selection (AIC-based StepAIC). This procedure automatically adds and removes variables to find the smallest set of predictors that still explains the outcome well.\nKeep only the best predictors\nAfter selection, we kept only the strongest variables for heart attack risk. The final model focused on:\n\nCholesterol\nHeart.Rate\nTriglycerides\nSystolic (systolic blood pressure)\nNorthern.Hemisphere\n\nStandardize numeric features\nWe created a scaled copy of the training data and standardized all numeric predictors (mean = 0, SD = 1). This helps the model stay stable and makes the coefficients more comparable.\nRefit the final model\nWe retrained logistic regression using only the selected predictors and the standardized data.\n\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nLogistic Regression\n\n\n55%\n\n\n50%\n\n\n\n\n\n\n2. Naive Bayes\nHow we built the model\n\nWe trained a Naive Bayes classifier on the original dataset using only the variables that showed the strongest correlation with heart attack risk:\n\nCholesterol\nHeart.Rate\nTriglycerides\nSystolic (systolic blood pressure)\nNorthern.Hemisphere\n\nWe focused on these features to reduce noise and keep only the most informative predictors.\n\nNaive Bayes assumes that the input features are conditionally independent given the class label.\nThis makes it very fast to train and easy to interpret.\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nNaive Bayes\n\n\n55%\n\n\n50%\n\n\n\n\n\n\n3. k-Nearest Neighbors (k-NN)\nHow we built the model\n\nWe treated k-NN as a distance-based classifier: a new patient is labeled based on the most similar patients in the training set.\nWe prepared the data using feature normalization, since k-NN is sensitive to scale:\n\nFirst tried Min-Max normalization, which gave ~50.4% accuracy.\nThen used Z-score standardization (mean = 0, std = 1), which improved performance to ~51.7%.\n\nWe tuned the model by testing multiple values of k (the number of neighbors), using odd values from 1 to 99.\nWe selected the best k based on the lowest test error.\n\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nKNN\n\n\n56%\n\n\n51%\n\n\n\n\n\n\n4. Decision Tree\nHow we built the model\n\nWe trained a classification tree using Heart.Attack.Ris as the target.\nWe used rpart() to grow the full tree on the training data.\nWe then used cross-validation to find the best cp (complexity parameter), which helps control overfitting.\nAfter selecting the optimal cp, we pruned the tree using prune() to keep only the most useful splits.\nFinally, we predicted on both the training set and the test set, and calculated confusion matrices and accuracy.\n\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nDecision Tree\n\n\n68%\n\n\n64%\n\n\n\n\n\n\n5. Random Forest\nHow we built the model\n\nWe first fit a Random Forest on the full training dataset using all available predictors.\nWe then looked at the model’s feature importance to see which variables contributed most to predicting heart attack risk.\nWe performed hyperparameter tuning on mtry (the number of variables randomly selected at each split).\n\nWe chose the mtry value that produced the lowest out-of-bag (OOB) error.\n\nThe best setting was mtry = 3.\n\nFinally, we retrained the Random Forest using the most important features and the best mtry.\n\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nRandom Forest\n\n\n100%\n\n\n52%\n\n\n\n\n\n\n6. Support Vector Machine (SVM)\nHow we built the model\n\nWe prepared the input data for SVM by:\n\nConverting all binary variables to factors\nStandardizing numeric features using scale() so they are on a comparable range\n\nWe treated SVM as a margin-based classifier: it tries to find the best separating boundary between “At Risk” and “No Risk.”\n\nHyperparameter tuning\n\nWe tuned the model using a radial (RBF-style) kernel.\nWe searched over:\n\ngamma values: 0.5, 1, 2, 5\n\ncost values: 0.01, 0.1, 1\n\nWe selected the setting that produced the best validation accuracy.\nBest model: kernel = \"radial\", gamma = 5, cost = 0.01\n\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nRandom Forest\n\n\n67%\n\n\n64%\n\n\n\n\n\n\n\nTrained on Subset data\nBecause most of our full-population models had only moderate accuracy, we tested the idea that heart attack risk may behave differently by country. Some factors (diet, access to care, lifestyle, environment) are country-specific, so a single global model might be averaging over very different patterns.\nWe first identified countries with stronger correlation signals for heart attack risk in our data and selected four of them: Italy, Japan, United States, and China. Then we created country-specific subsets using the following process:\n\nFilter the training and test sets (data1.train / data1.test) to keep only rows from one country at a time.\nRemove dummy columns that are all zeros within that country (for example, other country indicators that no longer apply).\nRetrain and evaluate all six models separately for each country.\n\nBest performance by country – KNN\nAmong the models we re-ran on each country subset, k-NN performed best overall. Below are the k-NN test accuracies for each country:\n\n\n\n\nCountry\n\n\nK value\n\n\nTest Accuracy (k-NN)\n\n\n\n\n\n\nItaly\n\n\n89\n\n\n68%\n\n\n\n\nJapan\n\n\n7\n\n\n66%\n\n\n\n\nUnited States\n\n\n79\n\n\n49%\n\n\n\n\nChina\n\n\n1\n\n\n48%\n\n\n\n\n   \n\n\nCountry-specific risk drivers\nAfter training k-NN separately on each country, we looked at which features were most influential for predicting heart attack risk in that country.\nThe lists below show the top three factors for each country, along with the direction of the relationship:\n\nPositive value (+) → higher value is associated with higher predicted risk\nNegative value (–) → higher value is associated with lower predicted risk / protective effect\n\nUnited States\n\nPhysical.Activity.Days.Per.Week (−0.1806)\n\nSystolic.Blood.Pressure (+0.1449)\n\nIncome (−0.0952)\n\nInterpretation: Lower physical activity and higher systolic blood pressure are linked to higher risk. Income also appears in the model, suggesting social/economic factors may play a role.\nItaly\n\nBMI (+0.1694)\n\nPrevious.Heart.Problems (−0.1424)\n\nIncome (+0.0942)\n\nInterpretation: Higher BMI is associated with elevated risk. Prior heart problems still matter, but the negative sign suggests the way it’s encoded in this subset may reflect treated/managed cases.\nJapan\n\nPrevious.Heart.Problems (−0.3429)\n\nHeart.Rate (+0.1970)\n\nUnhealthy.Diet (−0.1574)\n\nInterpretation: Elevated heart rate shows up as a strong risk indicator. Diet pattern also contributes to the model’s decision.\nChina\n\nStress.Level (−0.1589)\n\nTriglycerides (+0.1219)\n\nCholesterol (+0.1023)\n\nInterpretation: Blood lipid measures (triglycerides, cholesterol) are important risk signals. Stress also appears in the model for this population.\n\n\n\nConclusion\nOur models suggest that predicting heart attack risk is possible, but still challenging with the data we have.\nAt the global level, the Support Vector Machine (SVM) was the top-performing model, reaching about 64% test accuracy. While this was higher than Logistic Regression, Random Forest, k-NN, and others on the full dataset, 64% is not strong enough for confident clinical use. In other words, the model can see some signal, but it is not yet reliable as a real-world screening tool.\nHowever, when we looked at specific countries instead of the entire population, we saw more promising results. After subsetting data for four higher-signal countries (Italy, Japan, United States, China), we retrained models within each country. In those country-specific tests, k-NN performed best, especially in Italy (68% accuracy) and Japan (66% accuracy). This suggests that heart attack risk drivers may be more consistent within a single country than across the world — lifestyle patterns, healthcare access, and physiology may interact differently by region.\nWe also examined the top factors associated with predicted risk in each country.\n\nIn Italy, the most important signals pointed toward BMI / obesity, income level, and history of heart problems, which suggests prevention could focus on weight management, access for lower-income groups, and follow-up care for previously high-risk patients.\n\nIn Japan, key drivers included heart rate, diet quality, and history of heart issues, which points toward monitoring cardiovascular stress, improving diet, and providing targeted care to known high-risk individuals.\n\nThese country-level insights are useful because they hint that intervention strategies may need to be localized, not one-size-fits-all.\n\n\nLimitations\nA few technical observations:\n\nk-NN did well on country-level subsets because it makes very few assumptions about the shape of the data. k-NN is a local, distance-based model — it can adapt to non-linear boundaries and cluster-like patterns. That’s helpful when risk depends on subtle combinations of lifestyle and clinical variables that vary by population.\nTree-based models behaved oddly.\nOur Random Forest reached 100% training accuracy but only about 52% test accuracy, which is a clear sign of overfitting. Even after tuning (e.g. choosing the best mtry), the model still memorized the training data more than it generalized. This likely reflects limitations in the dataset: synthetic or AI-generated structure, small effective sample size within subgroups, and noisy relationships between predictors and the target.\nLogistic Regression and Naive Bayes stayed around ~50–55% accuracy on the test set. They were stable (little overfitting), but they couldn’t capture enough complexity to make strong predictions at the global scale.\n\n\nData limitation:\nData quality and source.\nThe dataset appears to be AI-generated / synthetic rather than clinical-grade EHR data. That means the relationships may not fully reflect real-world physiology, access to care, or reporting patterns.\nSample size within subgroups.\nOnce we subset by country, the number of usable rows per country dropped. That makes the models more sensitive to noise and increases the risk that the learned “signals” are unstable.\nGeneralizability.\nEven our best models (k-NN in Italy and Japan) still sit in the 60–70% accuracy range. These are promising for research but not medically actionable without further validation.\n\n\nBusiness Implication\nOur project suggests a practical way for healthcare organizations, insurers, and even employers to act before a heart attack happens instead of after. Rather than trying to use one global risk model for everyone, the results point toward building local models for specific populations (for example, by country or region) and using those models to flag people who are more likely to be at risk. Even if the model isn’t perfect, that early signal can be used to offer preventive outreach, coaching, screenings, or follow-up care to the right groups before an emergency occurs. That can mean better outcomes for patients, less strain on the healthcare system, and lower costs from avoidable cardiac events.\n\n\nNext steps\n\nGet better data.\nCollect larger, real-world health data (vitals, labs, reported behavior, history) from multiple populations. Ideally, include clinical labels confirmed by diagnosis.\nExpand geographic coverage.\nBuild and evaluate localized models for more countries and regions to see where population-specific risk modeling is strongest.\nImprove model tuning.\nExplore more systematic hyperparameter search (especially for SVM and Random Forest) and fairness / bias checks across demographic groups.\nMove beyond accuracy.\nFor real screening, we also care about recall (catching high-risk patients) and precision (avoiding false alarms). A model doesn’t need to be perfect to be useful — it needs to be good at prioritizing who should get follow-up.\n\nBottom line:\nA single global model struggled to capture heart attack risk reliably. But when we narrowed the problem to specific countries, accuracy improved and meaningful local risk drivers emerged. That suggests the path forward is not “one model for everyone,” but rather population-aware models + targeted prevention strategies.\n\n\nLinks"
  },
  {
    "objectID": "projects/job_rec.html",
    "href": "projects/job_rec.html",
    "title": "Job Recommendation System",
    "section": "",
    "text": "Overview\nThis project builds a skill based job recommendation system using a 1.3 million record LinkedIn jobs and skills dataset from 2024. The pipeline cleans and normalizes skills, represents text with TF-IDF, computes similarity to a users input profile, and returns a ranked list of relevant roles. The system shortens search time for candidates, helps recruiters target the right audience, and keeps job platforms engaging through accurate and transparent matches. Deliverables include a working prototype and a report that explains data preparation, model design, evaluation, and sample recommendations.\n\n\nProblem Statement\nRecruiters must screen large volumes of resumes for each opening, which leads to slow hiring cycles, inconsistent evaluations, and missed qualified candidates. Manual review struggles with unstructured resume formats, varied terminology, and subtle skill equivalence, and it increases the risk of unconscious bias and poor candidate experience.\nThe goal is to build a resume recommendation system that ingests a job description and a pool of resumes, then ranks and surfaces the most relevant candidates with clear reasons. The system should handle noisy text and diverse formats, map skills and experience to job requirements, and provide fast, explainable results that support fair and data driven decisions. Success will be measured by higher precision at the top of the ranked list, improved recall of qualified candidates, reduced time to screen, and stable fairness metrics across candidate groups.\n\n\nData\nThis project uses the LinkedIn Jobs and Skills dataset named “1.3M Linkedin Jobs & Skills (2024)” from Kaggle. The dataset contains about 1.3 million job listings from the 2024 with 2 fields. Each post has a link and a list of skills. For faster testing, we worked with a smaller sample of 50,000 rows and kept the full data for final checks. The file is loaded and cleaned in Google Colab.\nSnapshot\n\nRows in full set 1,296,381\n2 columns: job_link and job_skills\njob_link: the URL for the job post\njob_skills: a comma separated list of required or preferred skills\nWorking subset 50,000 rows\n\nData preprocessing\nCreate clean, consistent skills text that is ready for search, matching, and vectorization.\nSteps\n\nLoad and inspect read the CSV, check row count, columns, and missing values\n\nRemove noise drop rows with missing job_skills and remove duplicates so each job appears once\n\nNormalize text convert to lowercase, trim extra spaces, remove punctuation and special characters while keeping commas that separate skills\n\nTokenize skills split job_skills by commas so each skill becomes a clean token\n\nStopwords remove common words that add no meaning and a custom list such as skill, experience, degree\n\nLemmatize use spaCy to reduce words to their base form for example analyzing becomes analyze\n\nStandardize names build a skills mapping dictionary so variants map to one canonical name for example ms excel and microsoft excel become Excel\n\nReassemble join cleaned tokens into a standardized skills string for each job\n\nVectorize create Bag of Words and TF IDF features for ranking and retrieval, with embeddings as a future option\n\nQuality Check\n\nConfirm job_link remains unique\n\nRecount missing values after cleaning\n\nReview top skill frequencies before and after to verify standardization\n\nTime the pipeline on the 50,000 row sample to ensure efficient runs\n\n\n\nExploratory Data Analysis\n\n\n1. Wordcloud\n\nSoft skills dominate. Terms like communication, customer service, time management, and problem solving appear most often, signaling that employers value people who can interact well, manage time, and resolve issues across roles and industries.\n\nEducation signals. High school diploma appears frequently as a baseline; bachelor’s degree is present but less common, suggesting only some roles explicitly require higher education.\n\nDigital literacy. Tools such as Microsoft Office and Office suites show up strongly, indicating basic tech proficiency is a standard expectation.\n\nTakeaway: The market emphasizes transferable soft skills, basic digital tools, and a broad entry-level educational bar, which together shape how candidates should present skills and how platforms should weight features in recommendations.\n\n\n\n\n\n\n2. Top 20 Most Common Skills\n\nInterpersonal skills lead. Management, communication, and service appear most often, reinforcing the importance of collaboration and people skills across roles.\n\nIndustry-specific skills. Terms like medical, health, safety, and license point to healthcare and compliance needs where certifications are common.\n\nTechnical depth. System, engineering, and development highlight demand for technical expertise in software, infrastructure, and industrial roles.\n\nBusiness execution. Project, plan, teamwork, and analysis reflect strategic planning, delivery, and cross-functional collaboration.\n\nTakeaway: Employers want a mix of people skills, recognized credentials, technical capability, and analytical execution—with strong signals from healthcare, engineering, business management, insurance, and technology.\n\n\n\n\n\n\n3. Top 20 Most Common Skills in Analytics Roles\n\nCore soft skills. Analysis, management, and communication are prominent, showing analysts must interpret data and work effectively with teams.\n\nFinancial fluency. Financial, cost, budget, and accounting point to forecasting, cost analysis, budgeting, and reading financial statements to support decisions.\n\nTool basics. Microsoft and Excel signal baseline proficiency with spreadsheets and reporting workflows.\n\nOperational execution. Report, process, and system suggest compiling large datasets, standardizing processes, and communicating findings clearly.\n\nTakeaway: Analytics roles blend data interpretation, business finance, and clear communication, with everyday tooling in Excel and structured reporting.\n\n\n\n\n\n\nTools\nModeling and data: Python (Pandas, scikit learn, matplotlib, seaborn, re, nltk, spacy, wordcloud, matplotlib.pyplo, streamlit,t, flask, pyngrok)  Visualization and reporting: Excel  User interface: Bolt\n\n\nKey Methods and Approach\n1. Training pipeline\n\nBaseline TF-IDF + CosineBaseline Word2Vec + Clustering\n\n\n\nRepresent job skills and user profile with TF-IDF vectors\n\nRank jobs by cosine similarity to the user profile\n\nStrengths fast, transparent, easy to explain and tune\n\n\n\n\nEmbed skills with Word2Vec to capture semantic similarity\n\nCluster jobs and retrieve nearest clusters for the user profile\n\nStrengths handles synonyms and near-matches better than pure TF-IDF\n\n\n\n\n2. Hyperparameter tuning\n\nUse grid search, random search, or Bayesian optimization for key settings\n\nTF-IDF n-grams, min_df, max_df\n\nCosine retrieval top-K cutoff\n\nWord2Vec vector size, window, min_count, epochs\n\n\nTrack results with a simple experiment log for reproducibility\n\n3. Validation\n\nk-fold cross-validation for robust estimates\n\nHoldout set for a final unbiased check\n\nOptional A/B testing for UI-level comparisons once a prototype is live\n\n4. Model Evaluation Metrics\n\nPrecision, Recall, F1-score for relevance at the top of the list\n\nAUC-ROC for ranking quality (where labels exist)\n\nRMSE only when evaluating numeric relevance scores against ground truth\n\n5. Error analysis\n\nReview mis-ranked or missed jobs, and high-error segments\n\nCheck failure patterns by industry, seniority, and skill sparsity\n\nUse insights to refine cleaning, skill standardization, and feature choices\n\n6. User interface\n\nBolt UI with a LinkedIn-style layout for familiarity\n\nInputs user skill list and optional role or industry\n\nOutputs ranked jobs with top matching skills highlighted and a link to the posting\n\n7. Feedback loop\n\nCollect quick thumbs-up or thumbs-down to improve future rankings\n\nLog queries and outcomes to support retraining and threshold tuning\n\n\n\nModel Training\nGoal\nTrain and fine tune NLP models that deliver accurate, explainable job recommendations.\n1. Text vectorization (TF–IDF)\nWhat it is:\nVectorization turns text into numbers so models can learn. We considered One-Hot, Bag of Words, TF–IDF, and word embeddings (Word2Vec, GloVe, FastText). We use TF–IDF on the job_skills column.\nWhy TF–IDF:\n\nDown-weights common skills like “communication” or “teamwork,” reducing noisy matches\nStronger signal than BoW, which treats every token equally\nMore efficient than embeddings for our scope no large pre-trained models or extra compute\n\nSteps:\n\nBuild a TF–IDF model to convert job skills into numeric vectors\n\nGet the feature names (unique skills)\n\nTransform skills into a sparse TF–IDF matrix\n\nConvert to a Pandas DataFrame for ranking and retrieval\n\n2. Building the recommendation model:\nInputs:\n\nJobs LinkedIn postings from Kaggle with titles, descriptions, and skills\n\nResumes uploaded DOCX files with experience and skills\n\nPipelines:\n\nJob pipeline\n\nExtract title, description, and skills\n\nChunk long descriptions if needed\n\nVectorize with TF–IDF (and optional embeddings for future scale)\n\nStore job vectors in a vector database for fast lookup\n\n\nResume pipeline\n\nParse DOCX text and clean it\n\nNormalize skills to the same schema as jobs\n\nVectorize the resume to the same space as jobs\n\nStore resume vectors in the same index\n\n\nRetrieval and ranking:\n\nSimilarity metric use cosine similarity between the resume vector and job vectors\n\nRanking sort by similarity and return the top five matches with key skills highlighted\n\nFeedback loop:\n\nCollect thumbs-up or thumbs-down on recommendations\n\nRe-rank based on feedback and log interactions to improve future results\n\n3. Deployment and testing\nTest design:\nWe gathered a diverse set of resumes across multiple industries to check how well the system handles different backgrounds and wording. Sectors included Technical and Software, Healthcare and Medical, Education and Childcare, Creative and Design, and Business and Management.\nPrototype and UI:\nThe first UI was built with Bolt and deployed as a clickable prototype for rapid iteration:\nOpen the prototype\nUser test procedure:\n\nUpload resume (DOCX) into the mock UI\n\nExtract skills and normalize them to the same schema as job postings\n\nRetrieve matches using cosine similarity over the vector index\n\nReview Top-5 recommendations with matched skills highlighted\n\nRecord feedback on relevance and clarity for later re-ranking\n\nWhat we evaluated:\n\nSkill extraction accuracy do extracted skills reflect the resume content\n\nRecommendation quality are the top results relevant and varied\n\nUser experience is the flow clear, fast, and easy to use on first try\n\nExample walkthrough Healthcare resume\nWe uploaded a Healthcare and Medical resume and the system returned five recommended roles ranked by similarity. The first result was the closest match, with subsequent items offering adjacent options for exploration.\n\n\n\n\n\n\n\n\n\nResults\n\n\n\n\n\n\nNoteCurrent performance\n\n\n\nOffline accuracy: 85% on a held-out validation set (cross-validated).\n\n\nInitial findings\nEarly tests showed inaccurate recommendations. Two drivers stood out\n\nthe model was in an early stage\n\nthe Bolt prototype used a very small set of about fifteen job postings\n\nModel improvements\n\nAdded Word2Vec based matching to capture semantic similarity between skills\n\nExample cleaning and sanitation are treated as related, improving match quality\n\n\nKept TF–IDF for speed and transparency, and blended scores where helpful\n\nData freshness plan\n\nImplement periodic scraping of job listings with BeautifulSoup or Scrapy\n\nConsider API based feeds where available to reduce scraping overhead\n\nDefine a refresh schedule and deprecate stale jobs after a set age\n\nInterim augmentation (prototype phase)\n\nUsed manual uploads to add recent postings and diversify test data\n\nLogged outcomes to identify gaps in skills coverage and labeling\n\nWhat to track next\n\nClick-through and thumbs-up / thumbs-down rates in the UI\n\nCoverage of top skills by industry and the age of postings in the index\n\n\n\nBusiness Implication\nOur system shortens hiring cycles and lowers costs by automating first-pass screening, so recruiters spend time on fewer, higher-quality resumes. Using TF-IDF and Word2Vec to match skills and context improves shortlist precision and interview pass-through rates, while ongoing feedback tuning keeps results aligned with recruiter intent.\nBecause job listings are refreshed through automated scraping, recommendations stay current with market demand and scale cleanly as volume grows. The pipeline’s consistent text processing and explainable matching reduce subjective bias, supporting fairer, more defensible decisions. Candidates also benefit from clearer “why this match” signals and more relevant roles, which boosts engagement and reduces drop-off.\nFinally, product analytics—queries, matches, clicks, and feedback—surface skill gaps, channel performance, and conversion bottlenecks for workforce planning. Together, these capabilities differentiate the platform for both employers and job seekers. Core KPIs to monitor include time-to-screen, time-to-offer, precision@K, candidate response rate, cost per hire, and fairness metrics across groups.\n\n\nLinks"
  },
  {
    "objectID": "projects/heart.html#exploratory-data-analysis",
    "href": "projects/heart.html#exploratory-data-analysis",
    "title": "Heart Attack Prediction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nHeart Attack Risk by Country\n\n\n\n\nInsight\nThis chart shows the proportion of people labeled At Risk vs No Risk for heart attack in each country.\nThe overall pattern is very similar across countries: most countries have a similar at-risk share, around one third of the population.\nThis suggests that risk is widespread and not driven by one specific country in the dataset.\n\n\n\n\n\nDistribution of At-Risk Patients by Age\n\n\n\n\nInsight\nThis plot shows how many At Risk cases appear at different ages.\nRisk is not limited to only older adults: there is a noticeable number of at-risk individuals starting in their mid 20s and continuing through the 60s, 70s, and even 80s.\nIn other words, elevated risk appears across the full adult age range, not just in seniors.\n\n\n\n\n\nHeart Attack Risk by Gender\n\n\n\n\nInsight\nThe proportion of At Risk vs No Risk looks almost the same for males and females.\nThis suggests that in this dataset, heart attack risk is fairly balanced across gender — there is no large gap in baseline risk between men and women once they are in the sample."
  },
  {
    "objectID": "projects/heart.html#methods-and-approach",
    "href": "projects/heart.html#methods-and-approach",
    "title": "Heart Attack Prediction",
    "section": "Methods and Approach",
    "text": "Methods and Approach\nTo predict heart attack risk, we trained and compared six different classification models.\nOur goal was to identify the model that performs best on unseen data while staying clinically interpretable.\nWe evaluated each model primarily on test accuracy, using the same train/test split strategy described above.\n\nModels evaluated\n1. Logistic Regression\nA linear, probability-based classifier. It estimates the chance that a patient is “At Risk” given their features.\nStrengths: simple, interpretable, easy to explain to clinicians.\nLimitation: assumes a mostly linear relationship between predictors and risk.\n2. Discriminant Analysis (LDA / QDA)\nA statistical classifier that models how each class is distributed, then finds the boundary between them.\nLDA assumes classes are separated roughly linearly with shared variance, while QDA allows a curved (quadratic) boundary.\nStrengths: mathematically efficient and fast.\nLimitation: relies on distributional assumptions (normality, covariance structure).\n3. k-Nearest Neighbors (k-NN)\nA distance-based model. For a new patient, it looks at the most similar patients in the training set and votes.\nStrengths: non-parametric, can capture local patterns.\nLimitations: can be memory-intensive and sensitive to scaling; performance drops in high dimensions if features are noisy.\n4. Decision Tree\nA rule-based model that splits patients into groups using yes/no questions (for example, “Is systolic &gt; X?”).\nStrengths: highly interpretable, mirrors clinical reasoning.\nLimitation: can overfit if not pruned.\n5. Random Forest\nAn ensemble of many decision trees that vote together.\nStrengths: higher accuracy and more stability than a single tree; reduces overfitting.\nLimitation: less interpretable than a single tree, but still allows feature importance analysis.\n6. Support Vector Machine (SVM)\nFinds the best separating boundary (hyperplane) between “At Risk” and “No Risk,” and can use kernels to handle non-linear boundaries.\nStrengths: strong performance in high-dimensional feature spaces.\nLimitation: less intuitive to explain clinically compared to logistic regression or trees.\n\n\nModel selection\nEach model was trained on the balanced training set and tested on the realistic test set (with the original class ratio).\nWe compared their test accuracy side by side to understand which approach generalizes best.\nThat final comparison guides which model is most promising for deployment and clinical interpretation."
  },
  {
    "objectID": "projects/heart.html#models-evaluated",
    "href": "projects/heart.html#models-evaluated",
    "title": "Heart Attack Prediction",
    "section": "Models evaluated",
    "text": "Models evaluated\n1. Logistic Regression\nA linear, probability-based classifier. It estimates the chance that a patient is “At Risk” given their features.\nStrengths: simple, interpretable, easy to explain to clinicians.\nLimitation: assumes a mostly linear relationship between predictors and risk.\n2. Discriminant Analysis (LDA / QDA)\nA statistical classifier that models how each class is distributed, then finds the boundary between them.\nLDA assumes classes are separated roughly linearly with shared variance, while QDA allows a curved (quadratic) boundary.\nStrengths: mathematically efficient and fast.\nLimitation: relies on distributional assumptions (normality, covariance structure).\n3. k-Nearest Neighbors (k-NN)\nA distance-based model. For a new patient, it looks at the most similar patients in the training set and votes.\nStrengths: non-parametric, can capture local patterns.\nLimitations: can be memory-intensive and sensitive to scaling; performance drops in high dimensions if features are noisy.\n4. Decision Tree\nA rule-based model that splits patients into groups using yes/no questions (for example, “Is systolic &gt; X?”).\nStrengths: highly interpretable, mirrors clinical reasoning.\nLimitation: can overfit if not pruned.\n5. Random Forest\nAn ensemble of many decision trees that vote together.\nStrengths: higher accuracy and more stability than a single tree; reduces overfitting.\nLimitation: less interpretable than a single tree, but still allows feature importance analysis.\n6. Support Vector Machine (SVM)\nFinds the best separating boundary (hyperplane) between “At Risk” and “No Risk,” and can use kernels to handle non-linear boundaries.\nStrengths: strong performance in high-dimensional feature spaces.\nLimitation: less intuitive to explain clinically compared to logistic regression or trees.\n\nModel selection\nEach model was trained on the balanced training set and tested on the realistic test set (with the original class ratio).\nWe compared their test accuracy side by side to understand which approach generalizes best.\nThat final comparison guides which model is most promising for deployment and clinical interpretation."
  },
  {
    "objectID": "projects/heart.html#logistic-regression",
    "href": "projects/heart.html#logistic-regression",
    "title": "Heart Attack Prediction",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nHow we built the model\n\nStart with all features\nWe first fit a full logistic regression model on data.train using every available predictor.\nSelect the most important predictors\nWe ran stepwise feature selection (AIC-based StepAIC). This procedure automatically adds and removes variables to find the smallest set of predictors that still explains the outcome well.\nKeep only the best predictors\nAfter selection, we kept only the strongest variables for heart attack risk. The final model focused on:\n\nCholesterol\nHeart.Rate\nTriglycerides\nSystolic (systolic blood pressure)\nNorthern.Hemisphere\n\nStandardize numeric features\nWe created a scaled copy of the training data and standardized all numeric predictors (mean = 0, SD = 1). This helps the model stay stable and makes the coefficients more comparable.\nRefit the final model\nWe retrained logistic regression using only the selected predictors and the standardized data.\n\n\n\nPerformance\n```{=html}\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nLogistic Regression\n\n\n55%\n\n\n50%"
  },
  {
    "objectID": "projects/heart.html#naive-bayes",
    "href": "projects/heart.html#naive-bayes",
    "title": "Heart Attack Prediction",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nHow we built the model\n\nWe trained a Naive Bayes classifier on the original dataset using only the variables that showed the strongest correlation with heart attack risk:\n\nCholesterol\nHeart.Rate\nTriglycerides\nSystolic (systolic blood pressure)\nNorthern.Hemisphere\n\nWe focused on these features to reduce noise and keep only the most informative predictors.\n\nNaive Bayes assumes that the input features are conditionally independent given the class label.\nThis makes it very fast to train and easy to interpret.\n\n\nPerformance\n```{=html}\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nNaive Bayes\n\n\n55%\n\n\n50%"
  },
  {
    "objectID": "projects/heart.html#k-nearest-neighbors-k-nn",
    "href": "projects/heart.html#k-nearest-neighbors-k-nn",
    "title": "Heart Attack Prediction",
    "section": "k-Nearest Neighbors (k-NN)",
    "text": "k-Nearest Neighbors (k-NN)\n\nHow we built the model\n\nWe treated k-NN as a distance-based classifier: a new patient is labeled based on the most similar patients in the training set.\nWe prepared the data using feature normalization, since k-NN is sensitive to scale:\n\nFirst tried Min-Max normalization, which gave ~50.4% accuracy.\nThen used Z-score standardization (mean = 0, std = 1), which improved performance to ~51.7%.\n\nWe tuned the model by testing multiple values of k (the number of neighbors), using odd values from 1 to 99.\nWe selected the best k based on the lowest test error.\n\n\n\nPerformance\n```{=html}\n\n\n\n\n\n\nTest Accuracy\n\n\n\n\n\n\nk-NN (final tuned)\n\n\n~51.7%"
  },
  {
    "objectID": "projects/heart.html#decision-tree",
    "href": "projects/heart.html#decision-tree",
    "title": "Heart Attack Prediction",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nHow we built the model\n\nWe trained a classification tree using Heart.Attack.Risk as the target.\nWe used rpart() to grow the full tree on the training data.\nWe then used cross-validation to find the best cp (complexity parameter), which helps control overfitting.\nAfter selecting the optimal cp, we pruned the tree using prune() to keep only the most useful splits.\nFinally, we predicted on both the training set and the test set, and calculated confusion matrices and accuracy.\n\n\n\nPerformance\n```{=html}\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nDecision Tree\n\n\n68%\n\n\n64%"
  },
  {
    "objectID": "projects/heart.html#random-forest",
    "href": "projects/heart.html#random-forest",
    "title": "Heart Attack Prediction",
    "section": "Random Forest",
    "text": "Random Forest\n\nHow we built the model\n\nWe first fit a Random Forest on the full training dataset using all available predictors.\nWe then looked at the model’s feature importance to see which variables contributed most to predicting heart attack risk.\nWe performed hyperparameter tuning on mtry (the number of variables randomly selected at each split).\n\nWe chose the mtry value that produced the lowest out-of-bag (OOB) error.\n\nThe best setting was mtry = 3.\n\nFinally, we retrained the Random Forest using the most important features and the best mtry.\n\n\n\nPerformance\n```{=html}\n\n\n\n\n\n\nTrain Accuracy\n\n\nTest Accuracy\n\n\n\n\n\n\nRandom Forest\n\n\n100%\n\n\n52%"
  },
  {
    "objectID": "projects/heart.html#support-vector-machine-svm",
    "href": "projects/heart.html#support-vector-machine-svm",
    "title": "Heart Attack Prediction",
    "section": "Support Vector Machine (SVM)",
    "text": "Support Vector Machine (SVM)\n\nHow we built the model\n\nWe prepared the input data for SVM by:\n\nConverting all binary variables to factors\nStandardizing numeric features using scale() so they are on a comparable range\n\nWe treated SVM as a margin-based classifier: it tries to find the best separating boundary between “At Risk” and “No Risk.”\n\n\n\nHyperparameter tuning\n\nWe tuned the model using a radial (RBF-style) kernel.\nWe searched over:\n\ngamma values: 0.5, 1, 2, 5\n\ncost values: 0.01, 0.1, 1\n\nWe selected the setting that produced the best validation accuracy.\nBest model: kernel = \"radial\", gamma = 5, cost = 0.01"
  },
  {
    "objectID": "projects/heart.html#country-specific-risk-drivers",
    "href": "projects/heart.html#country-specific-risk-drivers",
    "title": "Heart Attack Prediction",
    "section": "Country-specific risk drivers",
    "text": "Country-specific risk drivers\nAfter training k-NN separately on each country, we looked at which features were most influential for predicting heart attack risk in that country.\nThe lists below show the top three factors for each country, along with the direction of the relationship: - Positive value (+) → higher value is associated with higher predicted risk - Negative value (–) → higher value is associated with lower predicted risk / protective effect\n\n\nUnited States 1. Physical.Activity.Days.Per.Week (−0.1806)\n2. Systolic.Blood.Pressure (+0.1449)\n3. Income (−0.0952)\n\nInterpretation: Lower physical activity and higher systolic blood pressure are linked to higher risk. Income also appears in the model, suggesting social/economic factors may play a role.\n\nItaly 1. BMI (+0.1694)\n2. Previous.Heart.Problems (−0.1424)\n3. Income (+0.0942)\n\nInterpretation: Higher BMI is associated with elevated risk. Prior heart problems still matter, but the negative sign suggests the way it’s encoded in this subset may reflect treated/managed cases.\n\nJapan 1. Previous.Heart.Problems (−0.3429)\n2. Heart.Rate (+0.1970)\n3. Unhealthy.Diet (−0.1574)\n\nInterpretation: Elevated heart rate shows up as a strong risk indicator. Diet pattern also contributes to the model’s decision.\n\nChina 1. Stress.Level (−0.1589)\n2. Triglycerides (+0.1219)\n3. Cholesterol (+0.1023)\n\nInterpretation: Blood lipid measures (triglycerides, cholesterol) are important risk signals. Stress also appears in the model for this population.\n\n\nConclusion"
  }
]